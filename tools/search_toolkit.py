"""
SearchAgent å·¥å…·åŒ…ï¼šåŸºäºä¼šè¯çš„æœç´¢ä¸æµè§ˆ (V5 - è®¤è¯ç‰ˆ)

èŒè´£ï¼šæä¾›äº’è”ç½‘æœç´¢å’Œç½‘é¡µæµè§ˆåŠŸèƒ½ï¼Œæ”¯æŒå¤æ‚çš„å¤šæ­¥éª¤ç½‘é¡µäº¤äº’å’Œç™»å½•çŠ¶æ€ã€‚

å·¥å…·åˆ—è¡¨ï¼š

=== æ ¸å¿ƒæœç´¢ (Core Search) ===
1. search(query: str, task_id: str) -> List[SearchResult]
   - æ‰§è¡Œäº’è”ç½‘æœç´¢ï¼Œä½¿ç”¨ Serper API
   - è¿”å›æ ‡é¢˜ã€URLã€æ‘˜è¦

=== ä¼šè¯ç®¡ç† (Session Management) ===
2. open_url(url: str, task_id: str, auth_context: Optional[str] = None) -> str
   - æ‰“å¼€ç½‘é¡µå¹¶è¿”å›ç®€åŒ–å†…å®¹ï¼ˆä½¿ç”¨ Playwrightï¼‰
   - ä¸º task_id åˆ›å»ºæˆ–é‡ç½®æŒä¹…æµè§ˆå™¨ä¼šè¯
   - ã€æ–°åŠŸèƒ½ã€‘æ”¯æŒé€šè¿‡ auth_context åŠ è½½ç™»å½•çŠ¶æ€
   - æ”¯æŒ PDF æ–‡ä»¶è‡ªåŠ¨æå–

3. close_browser_session(task_id: str) -> str
   - å…³é—­å¹¶æ¸…ç†æŒ‡å®š task_id çš„æµè§ˆå™¨ä¼šè¯
   - Agent å¿…é¡»åœ¨ä»»åŠ¡ç»“æŸæ—¶è°ƒç”¨æ­¤å·¥å…·é‡Šæ”¾èµ„æº

=== çŠ¶æ€ä¸å¯¼èˆª (State & Navigation) ===
4. get_current_url(task_id: str) -> str
   - è·å–æµè§ˆå™¨åœ°å€æ çš„å½“å‰ URL
   - ç”¨äºç¡®è®¤ä½ç½®å’Œæ„é€ å®Œæ•´è·¯å¾„
   
5. go_back(task_id: str) -> str
   - åé€€åˆ°ä¸Šä¸€é¡µï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨åé€€æŒ‰é’®ï¼‰
   - ç”¨äºé”™è¯¯æ¢å¤å’Œæ’¤é”€é”™è¯¯ç‚¹å‡»

6. scroll_page(direction: str, task_id: str) -> str
   - æ»šåŠ¨é¡µé¢ä»¥åŠ è½½æ›´å¤šå†…å®¹
   - è§¦å‘æ— é™æ»šåŠ¨å’Œæ‡’åŠ è½½

=== é¡µé¢äº¤äº’ (Page Interaction) ===
7. click_element(text_on_element: str, role: str, task_id: str) -> str
   - ç‚¹å‡»åŒ…å«æŒ‡å®šæ–‡æœ¬çš„å…ƒç´ ï¼ˆæ”¯æŒ link/button ç²¾ç¡®åŒºåˆ†ï¼‰
   - è‡ªåŠ¨å¤„ç†åŠ¨æ€å†…å®¹åŠ è½½ï¼ˆAJAX/XHRï¼‰
   
8. type_text_in_element(text_to_type: str, element_label_or_placeholder: str, task_id: str) -> str
   - åœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥æ–‡æœ¬
   - æ”¯æŒé€šè¿‡ label æˆ– placeholder å®šä½å…ƒç´ 

9. press_key(key: str, task_id: str) -> str
   - æ¨¡æ‹ŸæŒ‰é”®ï¼ˆå¦‚ "Enter" æäº¤æœç´¢ï¼‰
   - è‡ªåŠ¨ç­‰å¾…æ–°é¡µé¢åŠ è½½

=== é¡µé¢æ£€æŸ¥ (Page Inspection) ===
10. find_in_page(query: str, task_id: str) -> List[str]
    - åœ¨å½“å‰é¡µé¢ä¸­æœç´¢æ–‡æœ¬ï¼ˆæ¨¡æ‹Ÿ Ctrl+Fï¼‰
    - è¿”å›åŒ¹é…çš„æ–‡æœ¬ç‰‡æ®µåŠä¸Šä¸‹æ–‡

11. list_interactive_elements(task_id: str) -> List[Dict]
    - åˆ—å‡ºæ‰€æœ‰å¯äº¤äº’å…ƒç´ ï¼ˆé“¾æ¥å’ŒæŒ‰é’®ï¼‰çš„ç»“æ„åŒ–ä¿¡æ¯
    - è¿”å› textã€roleã€info å­—æ®µ

=== ç‰¹æ®ŠåŠŸèƒ½ (Special Features) ===
12. get_image_url(alt_text_query: str, task_id: str) -> str
    - æ ¹æ® alt_text æŸ¥æ‰¾å›¾ç‰‡å¹¶è¿”å› URL
    - SearchAgent å®šä½å›¾ç‰‡ï¼ŒMultimodalAgent åˆ†æå›¾ç‰‡

13. query_pdf_url(url: str, query: str, task_id: str) -> str
    - ã€æ–°ã€‘ä½¿ç”¨ RAG æŠ€æœ¯æŸ¥è¯¢ PDF æ–‡æ¡£
    - åªè¿”å›æœ€ç›¸å…³çš„ 3 ä¸ªæ–‡æœ¬ç‰‡æ®µï¼ˆé¿å…ä¸Šä¸‹æ–‡çª—å£çˆ†ç‚¸ï¼‰
    - å¿…é¡»ç”¨æ­¤å·¥å…·ä»£æ›¿ open_url å¤„ç† PDF
    - é€‚ç”¨äº ESG æŠ¥å‘Šã€ç ”ç©¶è®ºæ–‡ã€äº§å“æ‰‹å†Œç­‰å¤§å‹æ–‡æ¡£
"""

import os
import json
import asyncio
from pathlib import Path
from typing import Optional
from pydantic import Field
from oxygent.oxy import FunctionHub
from playwright.async_api import TimeoutError as PlaywrightTimeoutError
import requests

# å¯¼å…¥æ¨¡å—åŒ–ç»„ä»¶
from tools.search_toolkit_sub.browser_manager import session_manager
from tools.search_toolkit_sub.html_utils import (
    get_clean_page_content,
    save_page_content,
    save_search_results
)

# æ³¨å†Œæœç´¢å·¥å…·åŒ…
search_tools = FunctionHub(name="search_tools")

# ã€å…¨å±€ã€‘é¢„åŠ è½½åµŒå…¥æ¨¡å‹ï¼ˆé¿å…æ¯æ¬¡è°ƒç”¨ query_pdf_url æ—¶é‡æ–°åŠ è½½ï¼‰
_embedder_model = None

# ã€å…¨å±€ã€‘PDF åµŒå…¥ç¼“å­˜ï¼ˆæ ¸å¿ƒæ€§èƒ½ä¼˜åŒ–ï¼‰
# ç»“æ„: {url: {"embeddings": tensor, "pages": list, "timestamp": float}}
_pdf_embedding_cache = {}

# ã€é…ç½®ã€‘PDF å¤„ç†æŠ¤æ 
MAX_PAGES_TO_PROCESS = 150  # æœ€å¤§é¡µæ•°é™åˆ¶ï¼Œé˜²æ­¢å¤„ç†è¶…å¤§ PDF

def _get_embedder():
    """
    å»¶è¿ŸåŠ è½½å¹¶ç¼“å­˜åµŒå…¥æ¨¡å‹
    ã€æ€§èƒ½ä¼˜åŒ–ã€‘è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨ GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
    """
    global _embedder_model
    if _embedder_model is None:
        try:
            from sentence_transformers import SentenceTransformer
            import torch
            
            # æ£€æµ‹ GPU å¯ç”¨æ€§
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            print(f"ğŸ”„ é¦–æ¬¡åŠ è½½åµŒå…¥æ¨¡å‹ (paraphrase-multilingual-MiniLM-L12-v2)...")
            print(f"ğŸ–¥ï¸  ä½¿ç”¨è®¾å¤‡: {device.upper()}" + (" ğŸš€ [GPUåŠ é€Ÿ]" if device == 'cuda' else " [CPUæ¨¡å¼]"))
            
            _embedder_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2', device=device)
            print("âœ… åµŒå…¥æ¨¡å‹åŠ è½½å®Œæˆ")
        except ImportError:
            raise ImportError(
                "ç¼ºå°‘ 'sentence-transformers' åº“ã€‚è¯·å®‰è£…: pip install sentence-transformers"
            )
    return _embedder_model


# ==================== A. å¤–éƒ¨æœç´¢å·¥å…· (Stateless Function) ====================

@search_tools.tool(
    description="Search the internet for information using a search engine. "
    "Returns a list of search results with titles, URLs, and snippets. "
    "Use this tool when you need to find information about a topic, "
    "locate specific websites, or gather general knowledge."
)
async def search(
    query: str = Field(
        description="The search query string (e.g., 'äº¬ä¸œå¤§äº‹è®°', 'Python tutorial')"
    ),
    task_id: str = Field(
        description="Unique identifier for this task. Used for saving results."
    ),
) -> str:
    """
    æ‰§è¡Œäº’è”ç½‘æœç´¢ï¼ˆä½¿ç”¨ Serper APIï¼‰
    
    Args:
        query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²
        task_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦
    
    Returns:
        str: JSON æ ¼å¼çš„æœç´¢ç»“æœåˆ—è¡¨
    """
    print(f"ğŸ” æœç´¢: {query}")
    
    # æ£€æŸ¥ Serper API Key
    api_key = os.getenv("SERPER_API_KEY")
    if not api_key:
        error_msg = (
            "âš ï¸ Serper API æœªé…ç½®\n\n"
            "è¯·è®¾ç½®ç¯å¢ƒå˜é‡:\n"
            "export SERPER_API_KEY='your_api_key'\n\n"
            "æ³¨å†Œåœ°å€: https://serper.dev/"
        )
        return error_msg
    
    try:
        # è°ƒç”¨ Serper API
        url = "https://google.serper.dev/search"
        headers = {
            "X-API-KEY": api_key,
            "Content-Type": "application/json"
        }
        data = {
            "q": query,
            "num": 10,  # è¿”å› 10 æ¡ç»“æœ
        }
        
        response = requests.post(url, headers=headers, json=data, timeout=10)
        response.raise_for_status()
        result = response.json()
        
        # è§£æç»“æœ
        search_results = []
        
        # æœ‰æœºæœç´¢ç»“æœ
        for item in result.get("organic", []):
            search_results.append({
                "title": item.get("title", ""),
                "url": item.get("link", ""),
                "snippet": item.get("snippet", "")
            })
        
        # ä¿å­˜ç»“æœ
        save_search_results(task_id, query, search_results)
        
        print(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(search_results)} æ¡ç»“æœ")
        
        # è¿”å›æ ¼å¼åŒ–çš„ç»“æœ
        return json.dumps(search_results, ensure_ascii=False, indent=2)
        
    except requests.exceptions.RequestException as e:
        error_msg = f"âŒ æœç´¢å¤±è´¥: {str(e)}"
        print(error_msg)
        return error_msg
    except Exception as e:
        error_msg = f"âŒ æœç´¢å‡ºé”™: {str(e)}"
        print(error_msg)
        return error_msg


# ==================== B. ä¼šè¯ç®¡ç† (Session Management) ====================

@search_tools.tool(
    description="Open a web page and return its simplified content. "
    "Creates or resets a persistent browser session for this task_id. "
    "Uses a headless browser (Playwright) to handle JavaScript-rendered pages. "
    "ã€NEWã€‘Optional auth_context parameter (e.g., 'jd.com') automatically loads matching login state. "
    "ã€IMPORTANTã€‘If the URL is a PDF, this tool will guide you to use 'query_pdf_url' instead (to avoid context window explosion). "
    "Returns clean, readable text content (converted to Markdown) suitable for LLM processing. "
    "Essential for accessing dynamic websites and authenticated pages."
)
async def open_url(
    url: str = Field(
        description="The URL to open (e.g., 'https://item.jd.com/7307091.html' or 'https://example.com/document.pdf')"
    ),
    task_id: str = Field(
        description="Unique identifier for this task. Creates a persistent browser session for this task_id."
    ),
    auth_context: Optional[str] = Field(
        default=None,
        description="Optional authentication context key (e.g., 'jd.com', 'zhihu.com'). "
        "If provided, loads the corresponding login state (cookies + localStorage). "
        "If omitted or the file doesn't exist, opens as a guest. "
        "Use this when accessing pages that require login (e.g., 'My Orders', 'Shopping Cart')."
    ),
) -> str:
    """
    æ‰“å¼€ç½‘é¡µæˆ– PDF å¹¶è¿”å›ç®€åŒ–å†…å®¹
    
    ã€æ¶æ„æ ¸å¿ƒã€‘æ”¯æŒé€šè¿‡ auth_context åŠ è½½ç™»å½•çŠ¶æ€
    
    Args:
        url: è¦æ‰“å¼€çš„URLï¼ˆæ”¯æŒç½‘é¡µå’ŒPDFï¼‰
        task_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦
        auth_context: è®¤è¯ä¸Šä¸‹æ–‡å¯†é’¥ï¼ˆä¾‹å¦‚ "jd.com"ï¼‰
    
    Returns:
        str: ç®€åŒ–åçš„é¡µé¢å†…å®¹ï¼ˆMarkdownæ ¼å¼ï¼‰æˆ– PDF æ–‡æœ¬
    """
    print(f"ğŸŒ æ‰“å¼€URL: {url}")
    if auth_context:
        print(f"ğŸ” ä½¿ç”¨è®¤è¯ä¸Šä¸‹æ–‡: {auth_context}")
    
    # æ£€æµ‹æ˜¯å¦æ˜¯ PDF é“¾æ¥
    is_pdf = url.lower().endswith('.pdf') or '.pdf?' in url.lower()
    
    # ã€å…³é”®æ”¹è¿›ã€‘å¦‚æœæ˜¯ PDFï¼Œå¼•å¯¼ Agent ä½¿ç”¨ query_pdf_url å·¥å…·
    if is_pdf:
        return (
            "âš ï¸ æ£€æµ‹åˆ° PDF æ–‡ä»¶ã€‚\n\n"
            "ä¸ºäº†é¿å…ä¸Šä¸‹æ–‡çª—å£çˆ†ç‚¸ï¼Œè¯·ä½¿ç”¨ 'query_pdf_url' å·¥å…·æ¥æŸ¥è¯¢æ­¤ PDFï¼Œ"
            "è€Œä¸æ˜¯ä½¿ç”¨ 'open_url'ã€‚\n\n"
            f"å»ºè®®æ“ä½œï¼š\n"
            f"- å·¥å…·: query_pdf_url\n"
            f"- url: {url}\n"
            f"- query: [å°†æ‚¨çš„åŸå§‹é—®é¢˜ä½œä¸ºæŸ¥è¯¢å‚æ•°]\n"
            f"- task_id: {task_id}\n\n"
            f"ç¤ºä¾‹ï¼šå¦‚æœæ‚¨çš„ä»»åŠ¡æ˜¯'æ‰¾åˆ°äº¬ä¸œå¥åº·çš„ESGæ”¿ç­–æ•°é‡'ï¼Œåˆ™ query å‚æ•°åº”ä¸º "
            f"'äº¬ä¸œå¥åº·ä¸­æåˆ°çš„ESGæ”¿ç­–æ€»å…±æœ‰å‡ ä¸ªï¼Ÿ'"
        )
    
    # å¦åˆ™ä½¿ç”¨ Playwright å¤„ç†æ™®é€šç½‘é¡µ
    try:
        # ã€æ ¸å¿ƒå‡çº§ã€‘åˆ›å»ºæ–°ä¼šè¯ï¼Œæ”¯æŒ auth_context
        page = await session_manager.create_session(task_id, url, auth_context)
        
        # è·å–é¡µé¢å†…å®¹
        content = await get_clean_page_content(page)
        html = await page.content()
        
        # æ›´æ–°ä¼šè¯ç¼“å­˜
        session_manager.update_content(task_id, content, html)
        
        # ä¿å­˜å†…å®¹
        save_page_content(task_id, url, content, html)
        
        print(f"âœ… é¡µé¢å·²æ‰“å¼€ï¼Œå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        
        return content
        
    except PlaywrightTimeoutError:
        error_msg = f"âŒ æ‰“å¼€é¡µé¢è¶…æ—¶: {url}"
        print(error_msg)
        return error_msg
    except Exception as e:
        error_msg = f"âŒ æ‰“å¼€é¡µé¢å¤±è´¥: {str(e)}"
        print(error_msg)
        import traceback
        traceback.print_exc()
        return error_msg


@search_tools.tool(
    description="Close and cleanup the browser session for this task_id to release resources. "
    "CRITICAL: Agent MUST call this tool at the end of every task before returning the final answer. "
    "This ensures proper resource cleanup and prevents memory leaks."
)
async def close_browser_session(
    task_id: str = Field(
        description="Unique identifier for this task. The browser session associated with this task_id will be closed."
    ),
) -> str:
    """
    å…³é—­å¹¶æ¸…ç†æµè§ˆå™¨ä¼šè¯
    
    Args:
        task_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦
    
    Returns:
        str: ç¡®è®¤æ¶ˆæ¯
    """
    print(f"ğŸ”’ å…³é—­ä¼šè¯: {task_id[:8]}...")
    return await session_manager.close_session(task_id)


# ==================== C. çŠ¶æ€ä¸å¯¼èˆª (State & Navigation) ====================

@search_tools.tool(
    description="Get the current URL from the browser's address bar. "
    "Essential for confirming your location after navigation or redirects, "
    "and for constructing full URLs by appending file names to the current path. "
    "Use this when you need to know exactly where you are in the website."
)
async def get_current_url(
    task_id: str = Field(
        description="Unique identifier for this task."
    ),
) -> str:
    """
    è·å–å½“å‰æµè§ˆå™¨çš„ URL
    
    Args:
        task_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦
    
    Returns:
        str: å½“å‰é¡µé¢çš„å®Œæ•´ URL
    """
    print(f"ğŸ“ è·å–å½“å‰URL")
    
    try:
        page = await session_manager.get_session(task_id)
        
        if not page:
            return f"âŒ é”™è¯¯: ä¼šè¯ {task_id[:8]}... ä¸å­˜åœ¨ã€‚è¯·å…ˆä½¿ç”¨ open_url åˆ›å»ºä¼šè¯ã€‚"
        
        current_url = page.url
        print(f"âœ… å½“å‰URL: {current_url}")
        
        return current_url
        
    except Exception as e:
        error_msg = f"âŒ è·å–URLå¤±è´¥: {str(e)}"
        print(error_msg)
        return error_msg


@search_tools.tool(
    description="Go back to the previous page (simulates browser's back button). "
    "Critical for error recovery when you click a wrong link or enter a dead end. "
    "Returns the content of the previous page. "
    "Use this instead of calling open_url again when you need to undo navigation."
)
async def go_back(
    task_id: str = Field(
        description="Unique identifier for this task."
    ),
) -> str:
    """
    åé€€åˆ°ä¸Šä¸€é¡µ
    
    Args:
        task_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦
    
    Returns:
        str: åé€€åé¡µé¢çš„ç®€åŒ–å†…å®¹
    """
    print(f"â¬…ï¸ åé€€åˆ°ä¸Šä¸€é¡µ")
    
    try:
        page = await session_manager.get_session(task_id)
        
        if not page:
            return f"âŒ é”™è¯¯: ä¼šè¯ {task_id[:8]}... ä¸å­˜åœ¨ã€‚è¯·å…ˆä½¿ç”¨ open_url åˆ›å»ºä¼šè¯ã€‚"
        
        # æ‰§è¡Œåé€€æ“ä½œ
        await page.go_back(wait_until="networkidle", timeout=30000)
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        await asyncio.sleep(2)
        
        # è·å–é¡µé¢å†…å®¹
        content = await get_clean_page_content(page)
        html = await page.content()
        
        # æ›´æ–°ä¼šè¯ç¼“å­˜
        session_manager.update_content(task_id, content, html)
        
        # ä¿å­˜å†…å®¹
        current_url = page.url
        save_page_content(task_id, current_url, content, html)
        
        print(f"âœ… åé€€æˆåŠŸï¼Œå½“å‰URL: {current_url}")
        print(f"âœ… é¡µé¢å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        
        return content
        
    except Exception as e:
        error_msg = f"âŒ åé€€å¤±è´¥: {str(e)}"
        print(error_msg)
        return error_msg


@search_tools.tool(
    description="Scroll the current page up or down to load more content. "
    "Essential for infinite-scroll pages, lazy-loaded content, or long pages. "
    "Use this when you need to load older comments, more news items, or additional GitHub issues. "
    "Waits 3 seconds after scrolling for JS rendering. "
    "Returns the updated page content after scrolling."
)
async def scroll_page(
    direction: str = Field(
        default="down",
        description="Direction to scroll: 'down' or 'up'"
    ),
    task_id: str = Field(
        description="Unique identifier for this task."
    ),
) -> str:
    """
    æ»šåŠ¨é¡µé¢
    
    Args:
        direction: æ»šåŠ¨æ–¹å‘ ('down' æˆ– 'up')
        task_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦
    
    Returns:
        str: æ»šåŠ¨åçš„é¡µé¢å†…å®¹
    """
    print(f"ğŸ“œ æ»šåŠ¨é¡µé¢: {direction}")
    
    try:
        page = await session_manager.get_session(task_id)
        
        if not page:
            return f"âŒ é”™è¯¯: ä¼šè¯ {task_id[:8]}... ä¸å­˜åœ¨ã€‚è¯·å…ˆä½¿ç”¨ open_url åˆ›å»ºä¼šè¯ã€‚"
        
        # æ‰§è¡Œæ»šåŠ¨
        if direction.lower() == "down":
            # æ»šåŠ¨åˆ°åº•éƒ¨
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
        else:
            # æ»šåŠ¨åˆ°é¡¶éƒ¨
            await page.evaluate("window.scrollTo(0, 0)")
        
        # ã€å…³é”®ã€‘ç­‰å¾…æ–°å†…å®¹åŠ è½½ï¼ˆä¸º JS æ¸²æŸ“æä¾›æ—¶é—´ï¼‰
        await asyncio.sleep(3)
        
        # è·å–æ›´æ–°åçš„å†…å®¹
        content = await get_clean_page_content(page)
        html = await page.content()
        
        # æ›´æ–°ä¼šè¯ç¼“å­˜
        session_manager.update_content(task_id, content, html)
        
        # ä¿å­˜å†…å®¹
        current_url = page.url
        save_page_content(task_id, current_url, content, html)
        
        print(f"âœ… æ»šåŠ¨æˆåŠŸï¼Œæ–°å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        
        return content
        
    except Exception as e:
        error_msg = f"âŒ æ»šåŠ¨å¤±è´¥: {str(e)}"
        print(error_msg)
        return error_msg


# ==================== D. é¡µé¢äº¤äº’ (Page Interaction) ====================

@search_tools.tool(
    description="Click an element (link or button) on the current page by its text content AND role. "
    "This tool has PRECISE VISION - it can distinguish between elements with the same text but different roles. "
    "For example, it can click the 'main' link (folder) instead of the 'main' button (branch selector). "
    "CRITICAL FIX: Automatically handles dynamic content (AJAX/XHR) by waiting for network idle or 3 seconds. "
    "Use the exact 'text' and 'role' values from list_interactive_elements output. "
    "Role must be either 'link' or 'button'. Returns the new page content after clicking."
)
async def click_element(
    text_on_element: str = Field(
        description="The visible text on the element to click (e.g., 'å•†å“è¯„è®º', 'main', 'Files and versions')"
    ),
    role: str = Field(
        description="The role of the element: 'link' or 'button'. Get this from list_interactive_elements output."
    ),
    task_id: str = Field(
        description="Unique identifier for this task."
    ),
) -> str:
    """
    é€šè¿‡æ–‡æœ¬å’Œè§’è‰²ç²¾ç¡®ç‚¹å‡»é¡µé¢å…ƒç´ 
    
    Args:
        text_on_element: å…ƒç´ ä¸Šçš„æ–‡æœ¬
        role: å…ƒç´ çš„è§’è‰²ï¼ˆ'link' æˆ– 'button'ï¼‰
        task_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦
    
    Returns:
        str: ç‚¹å‡»åçš„æ–°é¡µé¢å†…å®¹
    """
    print(f"ğŸ‘† ç‚¹å‡»å…ƒç´ : '{text_on_element}' (role={role})")
    
    try:
        page = await session_manager.get_session(task_id)
        
        if not page:
            return f"âŒ é”™è¯¯: ä¼šè¯ {task_id[:8]}... ä¸å­˜åœ¨ã€‚è¯·å…ˆä½¿ç”¨ open_url åˆ›å»ºä¼šè¯ã€‚"
        
        # éªŒè¯ role å‚æ•°
        if role not in ['link', 'button']:
            return f"âŒ é”™è¯¯: role å¿…é¡»æ˜¯ 'link' æˆ– 'button'ï¼Œä½†å¾—åˆ°äº† '{role}'"
        
        clicked = False
        
        # ç­–ç•¥1: ä½¿ç”¨ Playwright çš„ get_by_role (æœ€ç²¾ç¡®)
        try:
            element = page.get_by_role(role, name=text_on_element, exact=True)
            await element.click(timeout=5000)
            clicked = True
            print(f"âœ… ä½¿ç”¨ç²¾ç¡® role+name åŒ¹é…æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ ç²¾ç¡® role+name åŒ¹é…å¤±è´¥: {e}")
        
        # ç­–ç•¥2: ä½¿ç”¨ get_by_role ä½†ä¸è¦æ±‚ç²¾ç¡®åŒ¹é…
        if not clicked:
            try:
                element = page.get_by_role(role, name=text_on_element)
                await element.first.click(timeout=5000)
                clicked = True
                print(f"âœ… ä½¿ç”¨æ¨¡ç³Š role+name åŒ¹é…æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ æ¨¡ç³Š role+name åŒ¹é…å¤±è´¥: {e}")
        
        # ç­–ç•¥3: ä½¿ç”¨ CSS é€‰æ‹©å™¨ + æ–‡æœ¬åŒ¹é…
        if not clicked:
            try:
                if role == 'link':
                    # æŸ¥æ‰¾æ‰€æœ‰ <a> æ ‡ç­¾
                    links = await page.query_selector_all("a")
                    for link in links:
                        link_text = await link.inner_text()
                        if link_text.strip() == text_on_element:
                            await link.click(timeout=5000)
                            clicked = True
                            print(f"âœ… ä½¿ç”¨ <a> æ ‡ç­¾åŒ¹é…æˆåŠŸ")
                            break
                elif role == 'button':
                    # æŸ¥æ‰¾æ‰€æœ‰ <button> æ ‡ç­¾å’Œ role="button" çš„å…ƒç´ 
                    buttons = await page.query_selector_all("button, [role='button']")
                    for button in buttons:
                        button_text = await button.inner_text()
                        if button_text.strip() == text_on_element:
                            await button.click(timeout=5000)
                            clicked = True
                            print(f"âœ… ä½¿ç”¨ button æ ‡ç­¾åŒ¹é…æˆåŠŸ")
                            break
            except Exception as e:
                print(f"âš ï¸ CSS é€‰æ‹©å™¨åŒ¹é…å¤±è´¥: {e}")
        
        # ç­–ç•¥4: å›é€€åˆ°æ—§çš„é€šç”¨ç­–ç•¥ï¼ˆä»…å½“å‰é¢éƒ½å¤±è´¥æ—¶ï¼‰
        if not clicked:
            try:
                print(f"âš ï¸ å°è¯•å›é€€åˆ°é€šç”¨æ–‡æœ¬åŒ¹é…")
                element = page.get_by_text(text_on_element, exact=True)
                await element.click(timeout=5000)
                clicked = True
                print(f"âœ… ä½¿ç”¨é€šç”¨æ–‡æœ¬åŒ¹é…æˆåŠŸï¼ˆä½†å¯èƒ½ç‚¹å‡»äº†é”™è¯¯çš„å…ƒç´ ï¼‰")
            except:
                pass
        
        if not clicked:
            return f"âŒ æœªæ‰¾åˆ°åŒ¹é…çš„å…ƒç´ : text='{text_on_element}', role='{role}'\nå»ºè®®ï¼šå…ˆè°ƒç”¨ list_interactive_elements ç¡®è®¤å…ƒç´ å­˜åœ¨"
        
        # ã€å…³é”®ä¿®å¤ã€‘æ™ºèƒ½ç­‰å¾…ï¼šå¤„ç†åŠ¨æ€å†…å®¹åŠ è½½
        try:
            # å°è¯•ç­‰å¾…é¡µé¢å¯¼èˆªï¼ˆå¦‚æœæ˜¯è·³è½¬é“¾æ¥ï¼‰
            await page.wait_for_load_state('networkidle', timeout=5000)
            print("âœ… æ£€æµ‹åˆ°é¡µé¢å¯¼èˆª")
        except PlaywrightTimeoutError:
            # è¶…æ—¶è¯´æ˜æ˜¯åŠ¨æ€å†…å®¹ï¼ˆAJAX/XHRï¼‰ï¼Œå›ºå®šç­‰å¾… 3 ç§’
            print("âš ï¸ æœªæ£€æµ‹åˆ°é¡µé¢å¯¼èˆªï¼Œå¯èƒ½æ˜¯åŠ¨æ€å†…å®¹ï¼Œç­‰å¾… 3 ç§’...")
            await asyncio.sleep(3)
        
        # è·å–æ–°é¡µé¢å†…å®¹
        content = await get_clean_page_content(page)
        html = await page.content()
        
        # æ›´æ–°ä¼šè¯ç¼“å­˜
        session_manager.update_content(task_id, content, html)
        
        # ä¿å­˜å†…å®¹
        current_url = page.url
        save_page_content(task_id, current_url, content, html)
        
        print(f"âœ… ç‚¹å‡»æˆåŠŸï¼Œæ–°é¡µé¢å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        
        return content
        
    except Exception as e:
        error_msg = f"âŒ ç‚¹å‡»å¤±è´¥: {str(e)}"
        print(error_msg)
        import traceback
        traceback.print_exc()
        return error_msg


@search_tools.tool(
    description="Type text into an input field on the current page. "
    "Identifies the input field by its label or placeholder text. "
    "Essential for using search boxes, filling forms, or entering queries. "
    "After typing, you typically need to call press_key('Enter') to submit."
)
async def type_text_in_element(
    text_to_type: str = Field(
        description="The text to type into the input field (e.g., 'numpy', 'gsm8k')"
    ),
    element_label_or_placeholder: str = Field(
        description="The label or placeholder text of the input field (e.g., 'Search issues', 'Username')"
    ),
    task_id: str = Field(
        description="Unique identifier for this task."
    ),
) -> str:
    """
    åœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥æ–‡æœ¬
    
    Args:
        text_to_type: è¦è¾“å…¥çš„æ–‡æœ¬
        element_label_or_placeholder: è¾“å…¥æ¡†çš„ label æˆ– placeholder
        task_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦
    
    Returns:
        str: å½“å‰é¡µé¢çš„å†…å®¹ï¼ˆè¾“å…¥é€šå¸¸ä¸åˆ·æ–°é¡µé¢ï¼‰
    """
    print(f"âŒ¨ï¸ è¾“å…¥æ–‡æœ¬: '{text_to_type}' åˆ° '{element_label_or_placeholder}'")
    
    try:
        page = await session_manager.get_session(task_id)
        
        if not page:
            return f"âŒ é”™è¯¯: ä¼šè¯ {task_id[:8]}... ä¸å­˜åœ¨ã€‚è¯·å…ˆä½¿ç”¨ open_url åˆ›å»ºä¼šè¯ã€‚"
        
        target = None
        
        # ç­–ç•¥1: é€šè¿‡ placeholder å®šä½
        try:
            target = page.get_by_placeholder(element_label_or_placeholder).first
            await target.fill(text_to_type, timeout=5000)
            print(f"âœ… ä½¿ç”¨ placeholder å®šä½æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ placeholder å®šä½å¤±è´¥: {e}")
        
        # ç­–ç•¥2: é€šè¿‡ label å®šä½
        if not target:
            try:
                target = page.get_by_label(element_label_or_placeholder).first
                await target.fill(text_to_type, timeout=5000)
                print(f"âœ… ä½¿ç”¨ label å®šä½æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ label å®šä½å¤±è´¥: {e}")
                return f"âŒ æœªæ‰¾åˆ°åŒ¹é…çš„è¾“å…¥æ¡†: label/placeholder='{element_label_or_placeholder}'"
        
        # ç­‰å¾…ä¸€ä¸‹ï¼ˆæŸäº›ç½‘ç«™ä¼šæœ‰è¾“å…¥å»¶è¿Ÿï¼‰
        await asyncio.sleep(1)
        
        # è·å–å½“å‰é¡µé¢å†…å®¹
        content = await get_clean_page_content(page)
        html = await page.content()
        
        # æ›´æ–°ä¼šè¯ç¼“å­˜
        session_manager.update_content(task_id, content, html)
        
        print(f"âœ… è¾“å…¥æˆåŠŸ")
        
        return content
        
    except Exception as e:
        error_msg = f"âŒ è¾“å…¥å¤±è´¥: {str(e)}"
        print(error_msg)
        import traceback
        traceback.print_exc()
        return error_msg


@search_tools.tool(
    description="Press a keyboard key (e.g., 'Enter') on the current page. "
    "Essential for submitting search queries after typing text. "
    "Automatically waits for new page to load after pressing Enter. "
    "Returns the new page content after key press."
)
async def press_key(
    key: str = Field(
        description="The key to press (e.g., 'Enter', 'Tab', 'Escape')"
    ),
    task_id: str = Field(
        description="Unique identifier for this task."
    ),
) -> str:
    """
    æ¨¡æ‹ŸæŒ‰é”®
    
    Args:
        key: è¦æŒ‰çš„é”®ï¼ˆå¦‚ 'Enter', 'Tab', 'Escape'ï¼‰
        task_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦
    
    Returns:
        str: æŒ‰é”®åçš„æ–°é¡µé¢å†…å®¹
    """
    print(f"âŒ¨ï¸ æŒ‰é”®: {key}")
    
    try:
        page = await session_manager.get_session(task_id)
        
        if not page:
            return f"âŒ é”™è¯¯: ä¼šè¯ {task_id[:8]}... ä¸å­˜åœ¨ã€‚è¯·å…ˆä½¿ç”¨ open_url åˆ›å»ºä¼šè¯ã€‚"
        
        # æŒ‰é”®
        await page.keyboard.press(key)
        
        # ã€å…³é”®ã€‘æŒ‰ Enter é€šå¸¸ä¼šè§¦å‘å¯¼èˆªï¼Œå¿…é¡»ç­‰å¾…
        if key.lower() == "enter":
            try:
                await page.wait_for_load_state('networkidle', timeout=15000)
                print("âœ… æ£€æµ‹åˆ°é¡µé¢å¯¼èˆª")
            except PlaywrightTimeoutError:
                # è¶…æ—¶åˆ™å›ºå®šç­‰å¾…
                print("âš ï¸ ç­‰å¾…è¶…æ—¶ï¼Œå›ºå®šç­‰å¾… 3 ç§’...")
                await asyncio.sleep(3)
        else:
            # å…¶ä»–æŒ‰é”®ç­‰å¾…çŸ­æš‚æ—¶é—´
            await asyncio.sleep(1)
        
        # è·å–æ–°é¡µé¢å†…å®¹
        content = await get_clean_page_content(page)
        html = await page.content()
        
        # æ›´æ–°ä¼šè¯ç¼“å­˜
        session_manager.update_content(task_id, content, html)
        
        # ä¿å­˜å†…å®¹
        current_url = page.url
        save_page_content(task_id, current_url, content, html)
        
        print(f"âœ… æŒ‰é”®æˆåŠŸï¼Œæ–°é¡µé¢å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        
        return content
        
    except Exception as e:
        error_msg = f"âŒ æŒ‰é”®å¤±è´¥: {str(e)}"
        print(error_msg)
        import traceback
        traceback.print_exc()
        return error_msg


# ==================== E. é¡µé¢æ£€æŸ¥ (Page Inspection) ====================

@search_tools.tool(
    description="Search for specific text in the currently opened page. "
    "Simulates browser's Ctrl+F functionality. Returns matching text snippets with context. "
    "Searches in the clean Markdown content, which preserves key-value relationships (e.g., 'å™ªéŸ³: 40db'). "
    "Use this tool to find specific information on the current page after opening it with open_url."
)
async def find_in_page(
    query: str = Field(
        description="The text to search for in the current page (e.g., 'å•†å“è¯„è®º', '2023-03-26', 'numpy')"
    ),
    task_id: str = Field(
        description="Unique identifier for this task."
    ),
) -> str:
    """
    åœ¨å½“å‰é¡µé¢ä¸­æœç´¢æ–‡æœ¬
    
    Args:
        query: è¦æœç´¢çš„æ–‡æœ¬
        task_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦
    
    Returns:
        str: JSONæ ¼å¼çš„åŒ¹é…ç»“æœåˆ—è¡¨
    """
    print(f"ğŸ” åœ¨å½“å‰é¡µé¢æœç´¢: {query}")
    
    content = session_manager.get_content(task_id)
    
    if not content:
        return f"âŒ é”™è¯¯: ä¼šè¯ {task_id[:8]}... æ²¡æœ‰å†…å®¹ã€‚è¯·å…ˆä½¿ç”¨ open_url æ‰“å¼€ä¸€ä¸ªé¡µé¢ã€‚"
    
    try:
        # åœ¨å†…å®¹ä¸­æœç´¢
        lines = content.split('\n')
        matches = []
        
        for i, line in enumerate(lines):
            if query.lower() in line.lower():
                # è·å–ä¸Šä¸‹æ–‡ï¼ˆå‰åå„2è¡Œï¼‰
                start = max(0, i - 2)
                end = min(len(lines), i + 3)
                context_lines = lines[start:end]
                
                matches.append({
                    "line_number": i + 1,
                    "matched_text": line.strip(),
                    "context": "\n".join(context_lines)
                })
        
        print(f"âœ… æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…")
        
        if not matches:
            return f"æœªæ‰¾åˆ°åŒ¹é… '{query}' çš„å†…å®¹"
        
        # é™åˆ¶è¿”å›æ•°é‡ï¼ˆé¿å…è¿‡é•¿ï¼‰
        if len(matches) > 20:
            matches = matches[:20]
            result = json.dumps(matches, ensure_ascii=False, indent=2)
            result += f"\n\n(ç»“æœå·²æˆªæ–­ï¼Œä»…æ˜¾ç¤ºå‰20ä¸ªåŒ¹é…)"
            return result
        
        return json.dumps(matches, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_msg = f"âŒ æœç´¢å¤±è´¥: {str(e)}"
        print(error_msg)
        return error_msg


@search_tools.tool(
    description="List all interactive elements (links and buttons) on the current page with structured information. "
    "This is your 'eyes' with COLOR VISION - it can distinguish between links and buttons with the same text. "
    "Returns a list of objects with 'text', 'role', and 'info' fields. "
    "Use this tool FIRST before clicking to see what's actually available and identify the correct element. "
    "Essential for eliminating blind clicking and avoiding confusion between elements with identical text."
)
async def list_interactive_elements(
    task_id: str = Field(
        description="Unique identifier for this task."
    ),
) -> str:
    """
    åˆ—å‡ºå½“å‰é¡µé¢ä¸Šæ‰€æœ‰å¯äº¤äº’å…ƒç´ çš„ç»“æ„åŒ–ä¿¡æ¯
    
    Args:
        task_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦
    
    Returns:
        str: JSONæ ¼å¼çš„å¯äº¤äº’å…ƒç´ åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« textã€roleã€info
    """
    print(f"ğŸ‘€ åˆ—å‡ºæ‰€æœ‰å¯äº¤äº’å…ƒç´ ï¼ˆç»“æ„åŒ–ï¼‰")
    
    try:
        page = await session_manager.get_session(task_id)
        
        if not page:
            return f"âŒ é”™è¯¯: ä¼šè¯ {task_id[:8]}... ä¸å­˜åœ¨ã€‚è¯·å…ˆä½¿ç”¨ open_url åˆ›å»ºä¼šè¯ã€‚"
        
        # è·å–æ‰€æœ‰é“¾æ¥å’ŒæŒ‰é’®ï¼ˆç»“æ„åŒ–ï¼‰
        interactive_elements = []
        
        # ç­–ç•¥1: è·å–æ‰€æœ‰é“¾æ¥ (a æ ‡ç­¾)
        links = await page.query_selector_all("a")
        for link in links:
            try:
                text = await link.inner_text()
                text = text.strip()
                if not text or len(text) == 0 or len(text) > 200:
                    continue
                
                # è·å–é¢å¤–ä¿¡æ¯
                href = await link.get_attribute("href") or ""
                title = await link.get_attribute("title") or ""
                aria_label = await link.get_attribute("aria-label") or ""
                
                # æ¨æ–­å…ƒç´ çš„ç”¨é€”
                info = ""
                if "folder" in href.lower() or "tree" in href.lower():
                    info = "Folder or directory"
                elif "file" in href.lower() or any(ext in href.lower() for ext in ['.parquet', '.json', '.csv', '.txt']):
                    info = "File"
                elif title:
                    info = title
                elif aria_label:
                    info = aria_label
                
                interactive_elements.append({
                    "text": text,
                    "role": "link",
                    "info": info
                })
            except:
                continue
        
        # ç­–ç•¥2: è·å–æ‰€æœ‰æŒ‰é’® (button æ ‡ç­¾)
        buttons = await page.query_selector_all("button")
        for button in buttons:
            try:
                text = await button.inner_text()
                text = text.strip()
                if not text or len(text) == 0 or len(text) > 200:
                    continue
                
                # è·å–é¢å¤–ä¿¡æ¯
                title = await button.get_attribute("title") or ""
                aria_label = await button.get_attribute("aria-label") or ""
                button_type = await button.get_attribute("type") or ""
                
                # æ¨æ–­å…ƒç´ çš„ç”¨é€”
                info = ""
                if "branch" in aria_label.lower() or "selector" in aria_label.lower():
                    info = "Branch selector"
                elif title:
                    info = title
                elif aria_label:
                    info = aria_label
                elif button_type:
                    info = f"Button ({button_type})"
                
                interactive_elements.append({
                    "text": text,
                    "role": "button",
                    "info": info
                })
            except:
                continue
        
        # ç­–ç•¥3: è·å–å…·æœ‰ role="button" çš„å…ƒç´ 
        role_buttons = await page.query_selector_all("[role='button']")
        for btn in role_buttons:
            try:
                # è·³è¿‡å·²ç»æ˜¯ button æ ‡ç­¾çš„å…ƒç´ ï¼ˆé¿å…é‡å¤ï¼‰
                tag_name = await btn.evaluate("element => element.tagName")
                if tag_name.lower() == "button":
                    continue
                
                text = await btn.inner_text()
                text = text.strip()
                if not text or len(text) == 0 or len(text) > 200:
                    continue
                
                aria_label = await btn.get_attribute("aria-label") or ""
                title = await btn.get_attribute("title") or ""
                
                info = aria_label or title or "Interactive element"
                
                interactive_elements.append({
                    "text": text,
                    "role": "button",
                    "info": info
                })
            except:
                continue
        
        print(f"âœ… æ‰¾åˆ° {len(interactive_elements)} ä¸ªå¯äº¤äº’å…ƒç´ ")
        
        # è¿”å›ç»“æ„åŒ–åˆ—è¡¨ï¼ˆä¸å»é‡ï¼Œå› ä¸ºåŒåå…ƒç´ çš„ role å¯èƒ½ä¸åŒï¼‰
        if not interactive_elements:
            return "âš ï¸ å½“å‰é¡µé¢æ²¡æœ‰æ‰¾åˆ°å¯äº¤äº’å…ƒç´ "
        
        # é™åˆ¶è¿”å›æ•°é‡ï¼ˆé¿å…è¿‡é•¿ï¼‰
        if len(interactive_elements) > 100:
            interactive_elements = interactive_elements[:100]
            result = json.dumps(interactive_elements, ensure_ascii=False, indent=2)
            result += "\n\n(åˆ—è¡¨å·²æˆªæ–­ï¼Œä»…æ˜¾ç¤ºå‰100ä¸ªå…ƒç´ )"
            return result
        
        return json.dumps(interactive_elements, ensure_ascii=False, indent=2)
        
    except Exception as e:
        error_msg = f"âŒ åˆ—å‡ºå…ƒç´ å¤±è´¥: {str(e)}"
        print(error_msg)
        return error_msg


@search_tools.tool(
    description="Find an image on the current page by its alt text or nearby text, and return its source URL. "
    "SearchAgent doesn't analyze images, but it can locate and extract image URLs. "
    "The URL can then be passed to multimodal_agent for image analysis. "
    "Use this when you need to find a specific image based on its description or context."
)
async def get_image_url(
    alt_text_query: str = Field(
        description="The alt text or nearby text to identify the image (e.g., 'äº§å“å›¾ç‰‡', 'logo')"
    ),
    task_id: str = Field(
        description="Unique identifier for this task."
    ),
) -> str:
    """
    æ ¹æ® alt_text æŸ¥æ‰¾å›¾ç‰‡å¹¶è¿”å› URL
    
    Args:
        alt_text_query: å›¾ç‰‡çš„ alt æ–‡æœ¬æˆ–ç›¸é‚»æ–‡æœ¬
        task_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦
    
    Returns:
        str: å›¾ç‰‡URL
    """
    print(f"ğŸ–¼ï¸ æŸ¥æ‰¾å›¾ç‰‡: {alt_text_query}")
    
    try:
        page = await session_manager.get_session(task_id)
        
        if not page:
            return f"âŒ é”™è¯¯: ä¼šè¯ {task_id[:8]}... ä¸å­˜åœ¨ã€‚è¯·å…ˆä½¿ç”¨ open_url åˆ›å»ºä¼šè¯ã€‚"
        
        # æŸ¥æ‰¾å›¾ç‰‡
        images = []
        
        # ç­–ç•¥1: é€šè¿‡ alt å±æ€§æŸ¥æ‰¾
        try:
            img_elements = await page.query_selector_all(f"img[alt*='{alt_text_query}']")
            for img in img_elements:
                src = await img.get_attribute("src")
                if src:
                    images.append(src)
        except:
            pass
        
        # ç­–ç•¥2: æŸ¥æ‰¾æ‰€æœ‰å›¾ç‰‡ï¼Œé€šè¿‡å‘¨å›´æ–‡æœ¬åˆ¤æ–­
        if not images:
            try:
                all_imgs = await page.query_selector_all("img")
                for img in all_imgs[:20]:  # é™åˆ¶æ•°é‡
                    alt = await img.get_attribute("alt") or ""
                    title = await img.get_attribute("title") or ""
                    src = await img.get_attribute("src") or ""
                    
                    if alt_text_query.lower() in alt.lower() or alt_text_query.lower() in title.lower():
                        if src:
                            images.append(src)
            except:
                pass
        
        if not images:
            return f"âŒ æœªæ‰¾åˆ°åŒ¹é… '{alt_text_query}' çš„å›¾ç‰‡"
        
        # è¿”å›ç¬¬ä¸€ä¸ªåŒ¹é…çš„å›¾ç‰‡URL
        image_url = images[0]
        
        # å¤„ç†ç›¸å¯¹URL
        if image_url.startswith("//"):
            image_url = "https:" + image_url
        elif image_url.startswith("/"):
            current_url = page.url
            from urllib.parse import urlparse
            parsed = urlparse(current_url)
            image_url = f"{parsed.scheme}://{parsed.netloc}{image_url}"
        
        print(f"âœ… æ‰¾åˆ°å›¾ç‰‡: {image_url}")
        
        # ä¿å­˜ç»“æœ
        output_dir = Path("local_es_data")
        output_dir.mkdir(parents=True, exist_ok=True)
        result_file = output_dir / f"{task_id}_image_url.txt"
        try:
            with open(result_file, "w", encoding="utf-8") as f:
                f.write(f"=== Task ID: {task_id} ===\n\n")
                f.write(f"=== æŸ¥è¯¢ ===\n{alt_text_query}\n\n")
                f.write(f"=== å›¾ç‰‡URL ===\n{image_url}\n")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç»“æœå¤±è´¥: {e}")
        
        return image_url
        
    except Exception as e:
        error_msg = f"âŒ æŸ¥æ‰¾å›¾ç‰‡å¤±è´¥: {str(e)}"
        print(error_msg)
        return error_msg


@search_tools.tool(
    description="Download a PDF from a URL, process it, and answer a specific question based on its content using semantic search. "
    "This is a RAG (Retrieval-Augmented Generation) tool that extracts ONLY the relevant snippets from the PDF, not the full text. "
    "CRITICAL: Use this instead of 'open_url' when you find a PDF document (e.g., ESG reports, research papers, manuals). "
    "Returns the top 3 most relevant text snippets with page numbers and similarity scores. "
    "Essential for efficiently extracting specific information from large documents without context window explosion."
)
async def query_pdf_url(
    url: str = Field(
        description="The URL of the PDF document (e.g., 'https://example.com/report-2024.pdf')"
    ),
    query: str = Field(
        description="The specific question to answer based on the PDF content. "
        "Pass your ORIGINAL TASK QUESTION here (e.g., 'How many ESG policies does JD Health mention?', "
        "'What is the noise level of the product?'). "
        "The tool will find the most relevant pages automatically."
    ),
    task_id: str = Field(
        description="Unique identifier for this task. Used for saving results."
    ),
) -> str:
    """
    ä» PDF URL ä¸‹è½½ã€å¤„ç†å¹¶æŸ¥è¯¢å†…å®¹ï¼ˆè½»é‡çº§ RAG - Retrieval-Augmented Generationï¼‰
    
    ã€æ ¸å¿ƒä¼˜åŠ¿ã€‘
    - åªè¿”å›ç›¸å…³ç‰‡æ®µï¼ˆtop 3ï¼‰ï¼Œè€Œä¸æ˜¯å…¨é‡æ–‡æœ¬
    - é¿å…ä¸Šä¸‹æ–‡çª—å£çˆ†ç‚¸
    - é™ä½ Token æˆæœ¬
    - æé«˜å‡†ç¡®æ€§ï¼ˆå‡å°‘å™ªå£°ï¼‰
    
    ã€æ€§èƒ½ä¼˜åŒ–ã€‘ï¼ˆV2 ç‰ˆæœ¬ï¼‰
    - âœ… æ·»åŠ æŠ¤æ ï¼šæœ€å¤§ 150 é¡µé™åˆ¶
    - âœ… æ™ºèƒ½ç¼“å­˜ï¼šåŒä¸€ PDF çš„åç»­æŸ¥è¯¢ä¸ºæ¯«ç§’çº§
    - âœ… GPU åŠ é€Ÿï¼šè‡ªåŠ¨ä½¿ç”¨ GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
    
    Args:
        url: PDF æ–‡ä»¶çš„ URL
        query: è¦æŸ¥è¯¢çš„å…·ä½“é—®é¢˜ï¼ˆä½¿ç”¨åŸå§‹ä»»åŠ¡é—®é¢˜ï¼‰
        task_id: ä»»åŠ¡å”¯ä¸€æ ‡è¯†ç¬¦
    
    Returns:
        str: å‰ 3 ä¸ªæœ€ç›¸å…³çš„æ–‡æœ¬ç‰‡æ®µï¼ˆåŒ…å«é¡µç å’Œç›¸ä¼¼åº¦åˆ†æ•°ï¼‰
    """
    print(f"ğŸ“„ æ­£åœ¨æŸ¥è¯¢ PDF: {url}")
    print(f"ğŸ” æŸ¥è¯¢é—®é¢˜: {query}")
    
    try:
        # ===== ã€æ€§èƒ½ä¼˜åŒ– 1ã€‘æ£€æŸ¥ç¼“å­˜ =====
        global _pdf_embedding_cache
        
        if url in _pdf_embedding_cache:
            print("âš¡ï¸ ç¼“å­˜å‘½ä¸­ï¼ä»ç¼“å­˜åŠ è½½åµŒå…¥ï¼ˆè·³è¿‡ä¸‹è½½ã€æå–ã€åµŒå…¥æ­¥éª¤ï¼‰...")
            corpus_embeddings = _pdf_embedding_cache[url]["embeddings"]
            extracted_pages = _pdf_embedding_cache[url]["pages"]
            print(f"âœ… ä»ç¼“å­˜åŠ è½½äº† {len(extracted_pages)} é¡µçš„åµŒå…¥å‘é‡")
        else:
            print("ğŸŒ ç¼“å­˜æœªå‘½ä¸­ï¼Œæ‰§è¡Œå®Œæ•´ PDF å¤„ç†...")
            
            # ===== æ­¥éª¤ 1: ä¸‹è½½ PDF =====
            print(f"ğŸ“¥ ä¸‹è½½ PDF...")
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            # ä¿å­˜ PDF æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            output_dir = Path("local_es_data")
            output_dir.mkdir(parents=True, exist_ok=True)
            pdf_file = output_dir / f"{task_id}_queried.pdf"
            
            with open(pdf_file, "wb") as f:
                f.write(response.content)
            
            print(f"âœ… PDF å·²ä¸‹è½½åˆ°: {pdf_file}")
            
            # ===== æ­¥éª¤ 2: æ–‡æœ¬æå–ï¼ˆæŒ‰é¡µï¼‰ =====
            print(f"ğŸ“– æå– PDF æ–‡æœ¬...")
            
            try:
                import pdfplumber
                from io import BytesIO
            except ImportError:
                error_msg = (
                    "âŒ ç¼ºå°‘ 'pdfplumber' åº“\n"
                    "è¯·å®‰è£…: pip install pdfplumber"
                )
                print(error_msg)
                return error_msg
            
            extracted_pages = []
            
            with pdfplumber.open(BytesIO(response.content)) as pdf:
                # ===== ã€æŠ¤æ  1ã€‘æ£€æŸ¥é¡µæ•°é™åˆ¶ =====
                total_pages = len(pdf.pages)
                
                if total_pages > MAX_PAGES_TO_PROCESS:
                    error_msg = (
                        f"âš ï¸ PDF å¤„ç†ä¸­æ­¢ï¼šæ–‡ä»¶æœ‰ {total_pages} é¡µï¼Œ"
                        f"è¶…è¿‡äº† {MAX_PAGES_TO_PROCESS} é¡µçš„é™åˆ¶ã€‚\n\n"
                        f"å»ºè®®ï¼š\n"
                        f"1. å°è¯•æ›´ç²¾ç¡®çš„æœç´¢ï¼Œæ‰¾åˆ°æ›´å°çš„æ–‡æ¡£\n"
                        f"2. å¦‚æœå¿…é¡»å¤„ç†æ­¤æ–‡æ¡£ï¼Œè¯·è”ç³»ç®¡ç†å‘˜æé«˜é¡µæ•°é™åˆ¶\n"
                        f"3. æˆ–è€…ä¸‹è½½æ–‡æ¡£å¹¶ä½¿ç”¨æœ¬åœ°å·¥å…·å¤„ç†"
                    )
                    print(error_msg)
                    return error_msg
                
                print(f"ğŸ“„ PDF æ€»é¡µæ•°: {total_pages} (åœ¨é™åˆ¶ {MAX_PAGES_TO_PROCESS} é¡µä»¥å†…)")
                
                # æå–æ–‡æœ¬
                for i, page in enumerate(pdf.pages, 1):
                    text = page.extract_text()
                    if text and text.strip():
                        extracted_pages.append({
                            "page": i,
                            "content": text.strip()
                        })
            
            if not extracted_pages:
                return "âš ï¸ PDF æ–‡æœ¬æå–ä¸ºç©ºï¼Œå¯èƒ½æ˜¯æ‰«æç‰ˆæˆ–å›¾ç‰‡ PDFã€‚å»ºè®®ä½¿ç”¨ multimodal_agent å¤„ç†æ­¤æ–‡æ¡£ã€‚"
            
            print(f"âœ… æå–äº† {len(extracted_pages)} é¡µæ–‡æœ¬")
            
            # ===== æ­¥éª¤ 3: å‘é‡åµŒå…¥ï¼ˆæ ¸å¿ƒ RAG é€»è¾‘ï¼‰ =====
            print(f"ğŸ” ç”Ÿæˆå‘é‡åµŒå…¥...")
            
            try:
                from sentence_transformers import util
                import time
                
                # è·å–é¢„åŠ è½½çš„åµŒå…¥æ¨¡å‹ï¼ˆæ”¯æŒ GPU åŠ é€Ÿï¼‰
                embedder = _get_embedder()
                
                # a. åµŒå…¥ PDF é¡µé¢å†…å®¹ï¼ˆè¿™æ˜¯æœ€è€—æ—¶çš„æ­¥éª¤ï¼‰
                start_time = time.time()
                corpus_embeddings = embedder.encode(
                    [page['content'] for page in extracted_pages], 
                    convert_to_tensor=True,
                    show_progress_bar=False
                )
                embedding_time = time.time() - start_time
                
                print(f"âœ… åµŒå…¥å®Œæˆï¼Œè€—æ—¶ {embedding_time:.2f} ç§’ ({len(extracted_pages)} é¡µ)")
                
                # ===== ã€æ€§èƒ½ä¼˜åŒ– 2ã€‘å­˜å‚¨åˆ°ç¼“å­˜ =====
                _pdf_embedding_cache[url] = {
                    "embeddings": corpus_embeddings,
                    "pages": extracted_pages,
                    "timestamp": time.time()
                }
                print(f"ğŸ’¾ å·²å°†åµŒå…¥ç¼“å­˜åˆ°å†…å­˜ï¼ˆç¼“å­˜å¤§å°: {len(_pdf_embedding_cache)} ä¸ª PDFï¼‰")
                
            except ImportError:
                error_msg = (
                    "âŒ ç¼ºå°‘ 'sentence-transformers' åº“\n"
                    "è¯·å®‰è£…: pip install sentence-transformers"
                )
                print(error_msg)
                return error_msg
        
        # ===== æ­¥éª¤ 4: æ‰§è¡Œè¯­ä¹‰æœç´¢ï¼ˆæ¯æ¬¡æŸ¥è¯¢éƒ½éœ€è¦ï¼‰ =====
        print(f"ğŸ” æ‰§è¡Œè¯­ä¹‰æœç´¢...")
        
        try:
            from sentence_transformers import util
            
            # è·å–åµŒå…¥æ¨¡å‹
            embedder = _get_embedder()
            
            # b. åµŒå…¥æŸ¥è¯¢ï¼ˆå¿«é€Ÿæ“ä½œï¼‰
            query_embedding = embedder.encode(
                query, 
                convert_to_tensor=True,
                show_progress_bar=False
            )
            
            # c. æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢ï¼ˆè¿”å› top 3ï¼‰
            hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=3)
            
        except ImportError:
            error_msg = (
                "âŒ ç¼ºå°‘ 'sentence-transformers' åº“\n"
                "è¯·å®‰è£…: pip install sentence-transformers"
            )
            print(error_msg)
            return error_msg
        
        # ===== æ­¥éª¤ 5: ç»„è£…å¹¶è¿”å›ç›¸å…³ç‰‡æ®µ =====
        relevant_snippets = []
        
        for hit in hits[0]:  # hits[0] æ˜¯ç¬¬ä¸€ä¸ªæŸ¥è¯¢çš„ç»“æœ
            page_data = extracted_pages[hit['corpus_id']]
            relevant_snippets.append(
                f"--- (ç›¸å…³ç‰‡æ®µ - æ¥è‡ªç¬¬ {page_data['page']} é¡µ, ç›¸ä¼¼åº¦: {hit['score']:.2f}) ---\n"
                f"{page_data['content']}\n"
            )
        
        final_response = (
            f"æ ¹æ®PDFæ–‡æ¡£ '{url}' å¯¹æŸ¥è¯¢ '{query}' çš„åˆ†æï¼Œæ‰¾åˆ°ä»¥ä¸‹é«˜ç›¸å…³æ€§ç‰‡æ®µï¼š\n\n"
            + "\n".join(relevant_snippets)
        )
        
        # ä¿å­˜æŸ¥è¯¢ç»“æœ
        output_dir = Path("local_es_data")
        output_dir.mkdir(parents=True, exist_ok=True)
        result_file = output_dir / f"{task_id}_pdf_query_result.txt"
        try:
            with open(result_file, "w", encoding="utf-8") as f:
                f.write(f"=== Task ID: {task_id} ===\n\n")
                f.write(f"=== PDF URL ===\n{url}\n\n")
                f.write(f"=== æŸ¥è¯¢ ===\n{query}\n\n")
                f.write(f"=== ç»“æœ ===\n{final_response}\n")
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç»“æœå¤±è´¥: {e}")
        
        print(f"âœ… PDF æŸ¥è¯¢å®Œæˆï¼Œè¿”å› {len(hits[0])} ä¸ªç›¸å…³ç‰‡æ®µã€‚")
        
        return final_response
        
    except requests.exceptions.RequestException as e:
        error_msg = f"âŒ ä¸‹è½½ PDF å¤±è´¥: {str(e)}"
        print(error_msg)
        return error_msg
        
    except Exception as e:
        error_msg = f"âŒ å¤„ç† PDF æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}"
        print(error_msg)
        import traceback
        traceback.print_exc()
        return error_msg


# ==================== F. ç¼“å­˜ç®¡ç†å·¥å…· (Cache Management Utilities) ====================

def get_pdf_cache_stats() -> dict:
    """
    è·å– PDF åµŒå…¥ç¼“å­˜çš„ç»Ÿè®¡ä¿¡æ¯
    
    Returns:
        dict: åŒ…å«ç¼“å­˜å¤§å°ã€URLåˆ—è¡¨ç­‰ä¿¡æ¯çš„å­—å…¸
    """
    global _pdf_embedding_cache
    
    return {
        "cache_size": len(_pdf_embedding_cache),
        "cached_urls": list(_pdf_embedding_cache.keys()),
        "total_pages_cached": sum(
            len(cache_data["pages"]) 
            for cache_data in _pdf_embedding_cache.values()
        )
    }


def clear_pdf_cache(url: Optional[str] = None) -> str:
    """
    æ¸…é™¤ PDF åµŒå…¥ç¼“å­˜
    
    Args:
        url: å¯é€‰ï¼ŒæŒ‡å®šè¦æ¸…é™¤çš„ PDF URLã€‚å¦‚æœä¸º Noneï¼Œåˆ™æ¸…é™¤æ‰€æœ‰ç¼“å­˜
    
    Returns:
        str: æ“ä½œç»“æœæ¶ˆæ¯
    """
    global _pdf_embedding_cache
    
    if url is None:
        # æ¸…é™¤æ‰€æœ‰ç¼“å­˜
        cache_size = len(_pdf_embedding_cache)
        _pdf_embedding_cache.clear()
        return f"âœ… å·²æ¸…é™¤æ‰€æœ‰ PDF ç¼“å­˜ï¼ˆå…± {cache_size} ä¸ªï¼‰"
    else:
        # æ¸…é™¤æŒ‡å®š URL çš„ç¼“å­˜
        if url in _pdf_embedding_cache:
            del _pdf_embedding_cache[url]
            return f"âœ… å·²æ¸…é™¤ PDF ç¼“å­˜: {url}"
        else:
            return f"âš ï¸ ç¼“å­˜ä¸­ä¸å­˜åœ¨æ­¤ URL: {url}"
