"""
VideoAgent å·¥å…·åŒ…ï¼šè§†é¢‘åˆ†æã€å¯¹è±¡æ£€æµ‹ã€å†…å®¹æœç´¢å’Œæµç¨‹åˆ†æ

èŒè´£ï¼šæä¾›å®Œæ•´çš„è§†é¢‘åˆ†æåŠŸèƒ½ï¼Œæ”¯æŒæ—¶åºå®šä½ã€CVæ£€æµ‹ã€OCR/ASRæœç´¢å’Œå¤æ‚æµç¨‹ç†è§£ã€‚

å·¥å…·åˆ—è¡¨ï¼š
1. get_video_metadata(file_path: str) -> dict
   - è·å–è§†é¢‘å…ƒæ•°æ®ï¼ˆæ—¶é•¿ã€åˆ†è¾¨ç‡ã€å¸§ç‡ï¼‰

2. detect_objects_in_video(file_path: str, object_prompt: str, timestamp: str, task_id: str) -> list[dict]
   - å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹ï¼ˆä½¿ç”¨ GroundingDINOï¼‰

3. track_user_events(file_path: str, task_id: str) -> list[dict]
   - è·Ÿè¸ªç”¨æˆ·äº¤äº’äº‹ä»¶ï¼ˆç‚¹å‡»ã€æ»‘åŠ¨ã€å…³æ³¨ç­‰ï¼‰

4. find_content_in_video(file_path: str, search_prompt: str, task_id: str) -> list[dict]
   - å¤šæ¨¡æ€å†…å®¹æœç´¢ï¼ˆASR + OCRï¼‰

5. analyze_video_flow(file_path: str, prompt: str, timestamp: str, task_id: str) -> str
   - å¤æ‚æµç¨‹åˆ†æï¼ˆä½¿ç”¨ VLMï¼‰
"""

import os
import cv2
import json
import asyncio
import numpy as np
from pathlib import Path
from typing import Optional, List, Dict, Any
from pydantic import Field
from oxygent.oxy import FunctionHub
import traceback

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
video_tools = FunctionHub(name="video_tools")


# ==================== å·¥å…· 1: è·å–è§†é¢‘å…ƒæ•°æ® ====================


@video_tools.tool(
    description="Get basic metadata from a video file, including duration (in seconds), "
    "resolution (width x height), and frame rate (fps). "
    "Use this tool when you need to know how long a video is or its technical specifications."
)
def get_video_metadata(
    file_path: str = Field(
        description="Path to the video file (e.g., './data/video.mp4')"
    ),
) -> dict:
    """
    è·å–è§†é¢‘æ–‡ä»¶çš„åŸºæœ¬å…ƒæ•°æ®

    Returns:
        dict: {"duration_seconds": 65.5, "resolution": "1920x1080", "fps": 30}
    """
    try:
        import ffmpeg

        print(f"ğŸ¬ æ­£åœ¨è·å–è§†é¢‘å…ƒæ•°æ®: {file_path}")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        video_file = Path(file_path)
        if not video_file.exists():
            return {"error": f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}

        # ä½¿ç”¨ ffmpeg è·å–å…ƒæ•°æ®
        probe = ffmpeg.probe(str(video_file))

        # æŸ¥æ‰¾è§†é¢‘æµ
        video_stream = next(
            (stream for stream in probe["streams"] if stream["codec_type"] == "video"),
            None,
        )

        if not video_stream:
            return {"error": "æ— æ³•æ‰¾åˆ°è§†é¢‘æµ"}

        # æå–å…ƒæ•°æ®
        duration = float(probe["format"]["duration"])
        width = int(video_stream["width"])
        height = int(video_stream["height"])

        # è®¡ç®—å¸§ç‡
        fps_parts = video_stream["r_frame_rate"].split("/")
        fps = int(fps_parts[0]) / int(fps_parts[1])

        metadata = {
            "duration_seconds": round(duration, 2),
            "resolution": f"{width}x{height}",
            "fps": round(fps, 2),
            "width": width,
            "height": height,
        }

        print(f"âœ… å…ƒæ•°æ®è·å–æˆåŠŸ: {metadata}")
        return metadata

    except ImportError:
        return {
            "error": "ç¼ºå°‘ä¾èµ–: ffmpeg-python",
            "install_hint": "pip install ffmpeg-python",
        }
    except Exception as e:
        return {"error": f"è·å–å…ƒæ•°æ®å¤±è´¥: {str(e)}"}


# ==================== å·¥å…· 2: å¼€æ”¾è¯æ±‡ç›®æ ‡æ£€æµ‹ ====================


@video_tools.tool(
    description="Detect and count visual objects in a video at a specific timestamp or range. "
    "Uses open-vocabulary object detection (GroundingDINO) to find objects based on natural language prompts. "
    "Examples: 'stars', 'activity badge', 'people', 'cameras'. "
    "Returns bounding boxes and counts of detected objects. "
    "Results will be saved to local_es_data/ directory."
)
def detect_objects_in_video(
    file_path: str = Field(
        description="Path to the video file (e.g., './data/video.mp4')"
    ),
    object_prompt: str = Field(
        description="Natural language description of the object to detect, "
        "e.g., 'stars', 'activity badge', 'people', 'phone cameras'"
    ),
    timestamp: str = Field(
        default="",
        description="Time point or range to analyze, e.g., '4s', '1m31s', '30s-32s'. "
        "Leave empty to search the entire video.",
    ),
    task_id: str = Field(
        description="Unique identifier for this task. Used for saving results."
    ),
) -> list:
    """
    åœ¨è§†é¢‘çš„æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ£€æµ‹è§†è§‰å¯¹è±¡ï¼ˆä½¿ç”¨ GroundingDINOï¼‰

    Returns:
        list[dict]: [{"timestamp_sec": 4.1, "detected_object": "æ˜Ÿæ˜Ÿ", "count": 3, "bounding_boxes": [...]}]
    """
    try:
        print(f"ğŸ¯ å¼€å§‹å¯¹è±¡æ£€æµ‹: {file_path}")
        print(f"   æ£€æµ‹ç›®æ ‡: {object_prompt}")
        print(f"   æ—¶é—´ç‚¹: {timestamp or 'å…¨è§†é¢‘'}")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        video_file = Path(file_path)
        if not video_file.exists():
            error_msg = f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            save_video_result(task_id, "detect_objects", file_path, error_msg)
            return [{"error": error_msg}]

        # è§£ææ—¶é—´æˆ³
        timestamps_to_check = _parse_timestamp(timestamp, file_path)

        # æå–å…³é”®å¸§
        frames_data = _extract_frames(file_path, timestamps_to_check)

        if not frames_data:
            error_msg = "æ— æ³•æå–è§†é¢‘å¸§"
            save_video_result(task_id, "detect_objects", file_path, error_msg)
            return [{"error": error_msg}]

        # ä½¿ç”¨ GroundingDINO æ£€æµ‹å¯¹è±¡
        results = []

        try:
            # å°è¯•å¯¼å…¥ GroundingDINO
            from groundingdino.util.inference import load_model, predict
            import torch

            # åŠ è½½æ¨¡å‹
            print("ğŸ“¥ åŠ è½½ GroundingDINO æ¨¡å‹...")
            model_config = "groundingdino/config/GroundingDINO_SwinT_OGC.py"
            model_checkpoint = "weights/groundingdino_swint_ogc.pth"

            # å¦‚æœæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæä¾›å¤‡é€‰æ–¹æ¡ˆ
            if not Path(model_checkpoint).exists():
                print("âš ï¸ GroundingDINO æ¨¡å‹æœªæ‰¾åˆ°ï¼Œä½¿ç”¨ç®€åŒ–æ£€æµ‹æ–¹æ³•")
                # å›é€€åˆ°ç®€å•çš„æ¨¡æ¿åŒ¹é…æˆ–å…¶ä»–æ–¹æ³•
                results = _fallback_object_detection(frames_data, object_prompt)
            else:
                model = load_model(model_config, model_checkpoint)

                # å¯¹æ¯ä¸€å¸§è¿›è¡Œæ£€æµ‹
                for frame_info in frames_data:
                    frame = frame_info["frame"]
                    timestamp_sec = frame_info["timestamp_sec"]

                    # æ‰§è¡Œæ£€æµ‹
                    boxes, logits, phrases = predict(
                        model=model,
                        image=frame,
                        caption=object_prompt,
                        box_threshold=0.35,
                        text_threshold=0.25,
                    )

                    # ä¿å­˜æ£€æµ‹ç»“æœ
                    if len(boxes) > 0:
                        # ä¿å­˜æ ‡æ³¨åçš„å¸§
                        annotated_frame = _draw_boxes(frame, boxes, phrases, logits)
                        frame_path = (
                            f"local_es_data/{task_id}_frame_{timestamp_sec:.1f}s.jpg"
                        )
                        cv2.imwrite(frame_path, annotated_frame)

                        result = {
                            "timestamp_sec": round(timestamp_sec, 2),
                            "detected_object": object_prompt,
                            "count": len(boxes),
                            "bounding_boxes": boxes.tolist(),
                            "confidence_scores": logits.tolist(),
                            "saved_frame": frame_path,
                        }
                        results.append(result)

                        print(f"âœ… åœ¨ {timestamp_sec:.1f}s æ£€æµ‹åˆ° {len(boxes)} ä¸ªå¯¹è±¡")

        except ImportError as e:
            print(f"âš ï¸ å¯¼å…¥ GroundingDINO å¤±è´¥: {e}")
            print("   ä½¿ç”¨ç®€åŒ–æ£€æµ‹æ–¹æ³•...")
            results = _fallback_object_detection(frames_data, object_prompt)

        # ä¿å­˜ç»“æœ
        if results:
            result_summary = json.dumps(results, ensure_ascii=False, indent=2)
            save_video_result(task_id, "detect_objects", file_path, result_summary)
            return results
        else:
            no_detection_msg = f"æœªæ£€æµ‹åˆ°å¯¹è±¡: {object_prompt}"
            save_video_result(task_id, "detect_objects", file_path, no_detection_msg)
            return [{"message": no_detection_msg, "count": 0}]

    except Exception as e:
        error_msg = f"å¯¹è±¡æ£€æµ‹å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        save_video_result(task_id, "detect_objects", file_path, error_msg)
        return [{"error": error_msg}]


# ==================== å·¥å…· 3: è·Ÿè¸ªç”¨æˆ·äº¤äº’äº‹ä»¶ ====================


@video_tools.tool(
    description="Track user interaction events in a screen recording video. "
    "Automatically detects clicks, swipes, and other interactions, and extracts context (button text, target items). "
    "Use this for tasks like 'what did the user add to cart' or 'which items did the user follow'. "
    "Results will be saved to local_es_data/ directory."
)
def track_user_events(
    file_path: str = Field(
        description="Path to the screen recording video file (e.g., './data/recording.mp4')"
    ),
    task_id: str = Field(
        description="Unique identifier for this task. Used for saving results."
    ),
) -> list:
    """
    è·Ÿè¸ªå±å¹•å½•åˆ¶è§†é¢‘ä¸­çš„ç”¨æˆ·äº¤äº’äº‹ä»¶

    Returns:
        list[dict]: [
            {"timestamp_sec": 12.5, "event_type": "click", "target_text": "åŠ å…¥è´­ç‰©è½¦", "context": "..."},
            {"timestamp_sec": 20.3, "event_type": "follow", "target_text": "å…³æ³¨", "context_text": "å•†å“A"}
        ]
    """
    try:
        print(f"ğŸ‘† å¼€å§‹è·Ÿè¸ªç”¨æˆ·äº‹ä»¶: {file_path}")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        video_file = Path(file_path)
        if not video_file.exists():
            error_msg = f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            save_video_result(task_id, "track_events", file_path, error_msg)
            return [{"error": error_msg}]

        # ä½¿ç”¨ VLM åˆ†ææ•´ä¸ªæµç¨‹ï¼ˆè¿™æ˜¯ä¸€ä¸ªå¤æ‚ä»»åŠ¡ï¼‰
        # å®é™…ä¸Šï¼Œè¿™ä¸ªå·¥å…·æœ€å¥½é€šè¿‡ analyze_video_flow å®ç°

        # ç®€åŒ–å®ç°ï¼šé‡‡æ ·å…³é”®å¸§ï¼Œä½¿ç”¨ OCR + å¸§å·®åˆ†æ
        events = []

        # 1. æå–è§†é¢‘å¸§åºåˆ—ï¼ˆå¯†é›†é‡‡æ ·ï¼šæ¯0.5ç§’ä¸€å¸§ï¼‰
        cap = cv2.VideoCapture(str(video_file))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps

        print(f"   è§†é¢‘æ—¶é•¿: {duration:.1f}s, FPS: {fps:.1f}")

        # é‡‡æ ·é—´éš”ï¼ˆ0.5ç§’ï¼‰
        sample_interval = 0.5
        frame_indices = []
        for t in np.arange(0, duration, sample_interval):
            frame_idx = int(t * fps)
            if frame_idx < total_frames:
                frame_indices.append(frame_idx)

        print(f"   é‡‡æ · {len(frame_indices)} å¸§...")

        # 2. æå–å¸§å¹¶æ£€æµ‹å˜åŒ–
        prev_frame = None
        for frame_idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()

            if not ret:
                continue

            timestamp_sec = frame_idx / fps

            # æ£€æµ‹å¸§å˜åŒ–ï¼ˆç®€å•çš„å¸§å·®åˆ†ï¼‰
            if prev_frame is not None:
                # è®¡ç®—å¸§å·®
                diff = cv2.absdiff(frame, prev_frame)
                diff_gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)
                change_ratio = np.sum(diff_gray > 30) / diff_gray.size

                # å¦‚æœå˜åŒ–æ˜¾è‘—ï¼ˆå¯èƒ½æ˜¯ç‚¹å‡»/è·³è½¬ï¼‰
                if change_ratio > 0.1:
                    # ä½¿ç”¨ OCR æå–æ–‡æœ¬
                    try:
                        from paddleocr import PaddleOCR

                        ocr = PaddleOCR(use_angle_cls=True, lang="ch")
                        ocr_result = ocr.predict(frame)

                        # æå–æ–‡æœ¬
                        texts = []
                        if ocr_result and ocr_result[0]:
                            for line in ocr_result[0]:
                                if line[1][1] > 0.5:  # ç½®ä¿¡åº¦é˜ˆå€¼
                                    texts.append(line[1][0])

                        # åˆ¤æ–­äº‹ä»¶ç±»å‹ï¼ˆåŸºäºOCRæ–‡æœ¬ï¼‰
                        event_type = "unknown"
                        target_text = ""

                        for text in texts:
                            if "åŠ å…¥è´­ç‰©è½¦" in text or "è´­ç‰©è½¦" in text:
                                event_type = "add_to_cart"
                                target_text = text
                                break
                            elif "å…³æ³¨" in text:
                                event_type = "follow"
                                target_text = text
                                break
                            elif "ç­›é€‰" in text:
                                event_type = "filter"
                                target_text = text
                                break
                            elif "ç‚¹å‡»" in text or "é€‰æ‹©" in text:
                                event_type = "click"
                                target_text = text
                                break

                        if event_type != "unknown":
                            # ä¿å­˜äº‹ä»¶å¸§
                            frame_path = f"local_es_data/{task_id}_event_{len(events)}_{timestamp_sec:.1f}s.jpg"
                            cv2.imwrite(frame_path, frame)

                            events.append(
                                {
                                    "timestamp_sec": round(timestamp_sec, 2),
                                    "event_type": event_type,
                                    "target_text": target_text,
                                    "context_texts": texts[
                                        :5
                                    ],  # ä¿å­˜å‰5ä¸ªæ–‡æœ¬ä½œä¸ºä¸Šä¸‹æ–‡
                                    "saved_frame": frame_path,
                                }
                            )

                            print(f"âœ… æ£€æµ‹åˆ°äº‹ä»¶: {event_type} @ {timestamp_sec:.1f}s")

                    except ImportError:
                        print("âš ï¸ PaddleOCR æœªå®‰è£…ï¼Œè·³è¿‡OCRåˆ†æ")
                    except Exception as e:
                        print(f"âš ï¸ OCR åˆ†æå¤±è´¥: {e}")

            prev_frame = frame.copy()

        cap.release()

        # ä¿å­˜ç»“æœ
        if events:
            result_summary = json.dumps(events, ensure_ascii=False, indent=2)
            save_video_result(task_id, "track_events", file_path, result_summary)
            print(f"âœ… å…±æ£€æµ‹åˆ° {len(events)} ä¸ªäº‹ä»¶")
            return events
        else:
            no_events_msg = "æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„ç”¨æˆ·äº¤äº’äº‹ä»¶"
            save_video_result(task_id, "track_events", file_path, no_events_msg)
            return [{"message": no_events_msg}]

    except Exception as e:
        error_msg = f"äº‹ä»¶è·Ÿè¸ªå¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        save_video_result(task_id, "track_events", file_path, error_msg)
        return [{"error": error_msg}]


# ==================== å·¥å…· 4: å¤šæ¨¡æ€å†…å®¹æœç´¢ ====================


@video_tools.tool(
    description="Search for multiple independent information entities in a video by analyzing both on-screen text (OCR) "
    "and spoken audio (ASR). The tool will search for each entity separately and return structured results. "
    "Examples: ['battery capacity', 'release date'], ['product model', 'price'], ['iPhone model', 'release month']. "
    "Results will be saved to local_es_data/ directory."
)
async def find_content_in_video(
    file_path: str = Field(
        description="Path to the video file (e.g., './data/video.mp4')"
    ),
    extraction_prompts: list = Field(
        description="A list of independent information entities to extract. "
        "The tool will search for each entity separately. "
        "e.g., ['product model', 'release month', 'battery capacity']"
    ),
    task_id: str = Field(
        description="Unique identifier for this task. Used for saving results."
    ),
) -> dict:
    """
    åœ¨è§†é¢‘ä¸­ç‹¬ç«‹æœç´¢å¤šä¸ªä¿¡æ¯å®ä½“ï¼ˆOCR + ASR åŒè½¨æœç´¢ï¼‰
    
    Args:
        file_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        extraction_prompts: è¦æå–çš„ä¿¡æ¯å®ä½“åˆ—è¡¨ï¼ˆæ¯ä¸ªç‹¬ç«‹æœç´¢ï¼‰
        task_id: ä»»åŠ¡ID
        
    Returns:
        dict[str, list]: {
            "iPhoneå‹å·": [{"timestamp_sec": 4.1, "source": "ocr", "match_text": "iPhone 14", ...}],
            "å‘å¸ƒæœˆä»½": [{"timestamp_sec": 15.3, "source": "audio", "match_text": "ä¹æœˆå‘å¸ƒ", ...}],
            ...
        }
    """
    try:
        print(f"ğŸ” å¼€å§‹å†…å®¹æœç´¢: {file_path}")
        print(f"   æœç´¢å®ä½“: {extraction_prompts}")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        video_file = Path(file_path)
        if not video_file.exists():
            error_msg = f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            save_video_result(task_id, "find_content", str(file_path), error_msg)
            return {"error": error_msg}

        # åˆ›å»ºä¸´æ—¶ç›®å½•
        temp_dir = Path("local_es_data") / "temp" / task_id
        temp_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–ç»“æœå­—å…¸ï¼šä¸ºæ¯ä¸ªæç¤ºè¯åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨
        results_dict = {prompt: [] for prompt in extraction_prompts}
        
        # ç”¨äºä¸´æ—¶å­˜å‚¨æ‰€æœ‰åŒ¹é…é¡¹ï¼ˆå¸¦æœ‰å¯¹åº”çš„ promptï¼‰
        all_matches = []

        # === 1. ASR è½¨é“ï¼šéŸ³é¢‘è½¬å½• ===
        print("ğŸ¤ å¼€å§‹ ASR å¤„ç†...")

        try:
            # æå–éŸ³é¢‘
            import ffmpeg

            audio_path = temp_dir / "audio.wav"

            stream = ffmpeg.input(str(video_file))
            stream = ffmpeg.output(
                stream, str(audio_path), acodec="pcm_s16le", ac=1, ar="16000"
            )
            ffmpeg.run(stream, overwrite_output=True, quiet=True)

            # ä½¿ç”¨ Faster Whisper è½¬å½•
            from faster_whisper import WhisperModel

            print("   åŠ è½½ Whisper æ¨¡å‹...")
            model = WhisperModel("large-v2", device="auto", compute_type="auto")

            print("   è½¬å½•ä¸­...")
            segments, info = model.transcribe(str(audio_path), beam_size=5)

            # å¯¹æ¯ä¸ª segment éå†æ‰€æœ‰çš„ extraction_promptsï¼Œç‹¬ç«‹åŒ¹é…
            for segment in segments:
                text = segment.text.strip()
                
                # å¯¹æ¯ä¸ªæç¤ºè¯ç‹¬ç«‹æ£€æŸ¥
                for prompt in extraction_prompts:
                    if _fuzzy_match(prompt, text):
                        match_item = {
                            "timestamp_sec": round(segment.start, 2),
                            "source": "audio",
                            "match_text": text,
                            "segment_start": round(segment.start, 2),
                            "segment_end": round(segment.end, 2),
                            "confidence": "high",
                            "matched_prompt": prompt,
                        }
                        all_matches.append(match_item)
                        print(f"âœ… [ASR] '{prompt}' åŒ¹é…: {text} @ {segment.start:.1f}s")

        except ImportError as e:
            print(f"âš ï¸ ASR ä¾èµ–ç¼ºå¤±: {e}")
        except Exception as e:
            print(f"âš ï¸ ASR å¤„ç†å¤±è´¥: {e}")

        # === 2. OCR è½¨é“ï¼šå±å¹•æ–‡å­—è¯†åˆ« ===
        print("ğŸ‘ï¸ å¼€å§‹ OCR å¤„ç†...")

        try:
            from paddleocr import PaddleOCR

            ocr = PaddleOCR(use_angle_cls=True, lang="ch")

            # é‡‡æ ·ç­–ç•¥ï¼šæ¯ç§’1å¸§
            cap = cv2.VideoCapture(str(video_file))
            fps = cap.get(cv2.CAP_PROP_FPS)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            duration = total_frames / fps
            sample_interval = 1.0  # æ¯ç§’é‡‡æ ·1å¸§

            for t in np.arange(0, duration, sample_interval):
                frame_idx = int(t * fps)
                if frame_idx >= total_frames:
                    break

                cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
                ret, frame = cap.read()

                if not ret:
                    continue

                ocr_result = ocr.predict(frame)

                # (ç­–ç•¥ 1) å¥å£®æ€§æ£€æŸ¥ + (ç­–ç•¥ 2) èšåˆæ¡†æ¶
                if ocr_result and ocr_result[0]:
                    frame_all_text_list = []
                    line_details = []

                    # --- ç¬¬ä¸€æ¬¡å¾ªç¯ï¼šå¥å£®åœ°æå–æ‰€æœ‰æ–‡æœ¬ ---
                    for line in ocr_result[0]:
                        try:
                            bbox = line[0]
                            text = line[1][0]
                            confidence = line[1][1]

                            if text and confidence > 0.5:
                                frame_all_text_list.append(text)
                                line_details.append(
                                    {
                                        "text": text,
                                        "confidence": confidence,
                                        "bbox": bbox,
                                    }
                                )

                        except (IndexError, TypeError, AttributeError):
                            # (ç­–ç•¥ 1) è·³è¿‡æŸåçš„è¡Œ
                            pass

                    if not frame_all_text_list:
                        continue

                    full_frame_text = " ".join(frame_all_text_list)

                    # --- ç¬¬äºŒæ­¥ï¼šå¯¹æ¯ä¸ª prompt ç‹¬ç«‹æ£€æŸ¥å…¨å¸§æ–‡æœ¬ ---
                    matched_prompts = []
                    for prompt in extraction_prompts:
                        if _fuzzy_match(prompt, full_frame_text):
                            matched_prompts.append(prompt)
                    
                    # å¦‚æœè‡³å°‘æœ‰ä¸€ä¸ª prompt åŒ¹é…æˆåŠŸ
                    if matched_prompts:
                        print(f"âœ… [OCR] å¸§åŒ¹é… @ {t:.1f}s (åŒ¹é…çš„æç¤ºè¯: {matched_prompts})")

                        frame_path = f"local_es_data/{task_id}_ocr_match_{t:.1f}s.jpg"
                        annotated_frame = frame.copy()

                        # åœ¨å›¾åƒä¸Šç»˜åˆ¶æ‰€æœ‰è¯†åˆ«åˆ°çš„æ¡†
                        for line_data in line_details:
                            pts = np.array(line_data["bbox"], np.int32).reshape(
                                (-1, 1, 2)
                            )
                            cv2.polylines(annotated_frame, [pts], True, (0, 255, 0), 2)

                        cv2.imwrite(frame_path, annotated_frame)

                        # ä¸ºæ¯ä¸ªåŒ¹é…çš„ prompt æ·»åŠ åŒ¹é…é¡¹
                        for matched_prompt in matched_prompts:
                            # å°†è¿™ä¸€å¸§è¯†åˆ«åˆ°çš„æ‰€æœ‰è¡Œéƒ½ä½œä¸ºåŒ¹é…é¡¹æ·»åŠ 
                            for line_data in line_details:
                                match_item = {
                                    "timestamp_sec": round(t, 2),
                                    "source": "ocr",
                                    "match_text": line_data["text"],
                                    "bounding_box": line_data["bbox"],
                                    "confidence": round(line_data["confidence"], 2),
                                    "saved_frame": frame_path,
                                    "full_frame_context": full_frame_text,
                                    "matched_prompt": matched_prompt,
                                }
                                all_matches.append(match_item)

            cap.release()

        except ImportError as e:
            print(f"âš ï¸ OCR ä¾èµ–ç¼ºå¤±: {e}")
        except Exception as e:
            print(f"âš ï¸ OCR å¤„ç†å¤±è´¥: {e}")
            traceback.print_exc()

        # å°† all_matches æŒ‰ prompt åˆ†ç»„åˆ° results_dict
        for match_item in all_matches:
            prompt = match_item.pop("matched_prompt")  # ç§»é™¤ matched_prompt å­—æ®µ
            results_dict[prompt].append(match_item)
        
        # å¯¹æ¯ä¸ª prompt çš„ç»“æœæŒ‰æ—¶é—´æ’åº
        for prompt in results_dict:
            results_dict[prompt].sort(key=lambda x: x["timestamp_sec"])

        # ç»Ÿè®¡æ€»åŒ¹é…æ•°
        total_matches = sum(len(matches) for matches in results_dict.values())
        
        # æ‰“å°æ¯ä¸ª prompt çš„åŒ¹é…æƒ…å†µ
        print(f"\nğŸ“Š æœç´¢ç»“æœæ±‡æ€»:")
        for prompt, matches in results_dict.items():
            if matches:
                print(f"   âœ… '{prompt}': {len(matches)} ä¸ªåŒ¹é…é¡¹")
            else:
                print(f"   âŒ '{prompt}': æœªæ‰¾åˆ°åŒ¹é…å†…å®¹")

        # ä¿å­˜ç»“æœ
        result_summary = json.dumps(results_dict, ensure_ascii=False, indent=2)
        save_video_result(task_id, "find_content", str(file_path), result_summary)
        
        if total_matches > 0:
            print(f"âœ… æ€»å…±æ‰¾åˆ° {total_matches} ä¸ªåŒ¹é…é¡¹")
        else:
            print(f"âš ï¸ æ‰€æœ‰æç¤ºè¯å‡æœªæ‰¾åˆ°åŒ¹é…å†…å®¹")
        
        return results_dict

    except Exception as e:
        error_msg = f"å†…å®¹æœç´¢å¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        traceback.print_exc()
        save_video_result(task_id, "find_content", str(file_path), error_msg)
        return {"error": error_msg}


# ==================== å·¥å…· 5: å¤æ‚æµç¨‹åˆ†æï¼ˆVLMï¼‰====================


@video_tools.tool(
    description="Analyze complex video workflows using a Vision-Language Model (VLM). "
    "Use this as a LAST RESORT for tasks that require multi-step reasoning, calculations, "
    "or conditional logic that simpler tools cannot handle. "
    "Examples: 'calculate the memory difference between two products', "
    "'track user flow and determine the final result'. "
    "Results will be saved to local_es_data/ directory."
)
def analyze_video_flow(
    file_path: str = Field(
        description="Path to the video file (e.g., './data/video.mp4')"
    ),
    prompt: str = Field(
        description="Detailed instruction for the VLM, describing the entire task. "
        "Be specific about what to look for, what to calculate, and what format to return."
    ),
    timestamp: str = Field(
        default="",
        description="Time range to analyze, e.g., '0s-30s'. Leave empty to analyze the entire video.",
    ),
    task_id: str = Field(
        description="Unique identifier for this task. Used for saving results."
    ),
) -> str:
    """
    ä½¿ç”¨ VLM åˆ†æå¤æ‚çš„è§†é¢‘æµç¨‹

    Returns:
        str: VLM çš„åˆ†æç»“æœ
    """
    try:
        print(f"ğŸ§  å¼€å§‹ VLM æµç¨‹åˆ†æ: {file_path}")
        print(f"   Prompt: {prompt[:100]}...")

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        video_file = Path(file_path)
        if not video_file.exists():
            error_msg = f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
            save_video_result(task_id, "analyze_flow", file_path, error_msg)
            return error_msg

        # æ£€æŸ¥ VLM API é…ç½®
        vlm_api_key = os.getenv("DEFAULT_VLM_API_KEY")
        vlm_base_url = os.getenv("DEFAULT_VLM_BASE_URL")
        vlm_model = os.getenv("DEFAULT_VLM_MODEL_NAME")

        if not all([vlm_api_key, vlm_base_url, vlm_model]):
            error_msg = (
                "âš ï¸ VLM API æœªé…ç½®\n\n"
                "è¯·è®¾ç½®ç¯å¢ƒå˜é‡:\n"
                "export DEFAULT_VLM_API_KEY='your_api_key'\n"
                "export DEFAULT_VLM_BASE_URL='your_base_url'\n"
                "export DEFAULT_VLM_MODEL_NAME='your_model_name'\n"
            )
            save_video_result(task_id, "analyze_flow", file_path, error_msg)
            return error_msg

        # 1. æå–å…³é”®å¸§ï¼ˆå¯†é›†é‡‡æ ·ï¼šæ¯0.5ç§’ï¼‰
        print("   æå–å…³é”®å¸§...")

        cap = cv2.VideoCapture(str(video_file))
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        duration = total_frames / fps

        # è§£ææ—¶é—´èŒƒå›´
        if timestamp:
            start_time, end_time = _parse_time_range(timestamp, duration)
        else:
            start_time, end_time = 0, duration

        # é‡‡æ ·é—´éš”ï¼ˆ0.5ç§’ï¼‰
        sample_interval = 0.5
        frames_base64 = []

        for t in np.arange(start_time, end_time, sample_interval):
            frame_idx = int(t * fps)
            if frame_idx >= total_frames:
                break

            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()

            if not ret:
                continue

            # è½¬æ¢ä¸º base64
            import base64

            _, buffer = cv2.imencode(".jpg", frame)
            frame_base64 = base64.b64encode(buffer).decode("utf-8")
            frames_base64.append({"timestamp": round(t, 2), "image": frame_base64})

            # é™åˆ¶æœ€å¤§å¸§æ•°ï¼ˆé¿å…è¯·æ±‚è¿‡å¤§ï¼‰
            if len(frames_base64) >= 60:  # æœ€å¤š60å¸§ï¼ˆçº¦30ç§’ï¼‰
                break

        cap.release()

        print(f"   æå–äº† {len(frames_base64)} å¸§")

        # 2. è°ƒç”¨ VLM API
        print("   è°ƒç”¨ VLM API...")

        try:
            import requests

            # æ„å»ºæ¶ˆæ¯ï¼ˆOpenAI å…¼å®¹æ ¼å¼ï¼‰
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": f"{prompt}\n\nè¯·æŒ‰æ—¶é—´é¡ºåºè§‚çœ‹ä»¥ä¸‹å¸§ï¼Œå¹¶å®Œæˆä»»åŠ¡ã€‚",
                        }
                    ],
                }
            ]

            # æ·»åŠ å›¾åƒ
            for frame_data in frames_base64:
                messages[0]["content"].append(
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{frame_data['image']}"
                        },
                    }
                )

            # å‘é€è¯·æ±‚
            response = requests.post(
                f"{vlm_base_url}/chat/completions",
                headers={
                    "Authorization": f"Bearer {vlm_api_key}",
                    "Content-Type": "application/json",
                },
                json={
                    "model": vlm_model,
                    "messages": messages,
                    "max_tokens": 2048,
                    "temperature": 0.1,
                },
                timeout=300,
            )

            response.raise_for_status()
            result = response.json()

            # æå–å›å¤
            answer = result["choices"][0]["message"]["content"]

            print(f"âœ… VLM åˆ†æå®Œæˆ")
            print(f"   ç»“æœ: {answer[:200]}...")

            # ä¿å­˜ç»“æœ
            save_video_result(task_id, "analyze_flow", file_path, answer)

            return answer

        except requests.exceptions.RequestException as e:
            error_msg = f"VLM API è°ƒç”¨å¤±è´¥: {str(e)}"
            print(f"âŒ {error_msg}")
            save_video_result(task_id, "analyze_flow", file_path, error_msg)
            return error_msg

    except Exception as e:
        error_msg = f"æµç¨‹åˆ†æå¤±è´¥: {str(e)}"
        print(f"âŒ {error_msg}")
        save_video_result(task_id, "analyze_flow", file_path, error_msg)
        return error_msg


# ==================== è¾…åŠ©å‡½æ•° ====================


def _parse_timestamp(timestamp: str, video_path: str) -> List[float]:
    """
    è§£ææ—¶é—´æˆ³å­—ç¬¦ä¸²ä¸ºç§’æ•°åˆ—è¡¨

    Examples:
        "4s" -> [4.0]
        "1m31s" -> [91.0]
        "30s-32s" -> [30.0, 31.0, 32.0]
        "" -> [0, 5, 10, 15, ...] (å…¨è§†é¢‘é‡‡æ ·)
    """
    if not timestamp:
        # å…¨è§†é¢‘é‡‡æ ·ï¼šæ¯5ç§’ä¸€å¸§
        metadata = get_video_metadata(video_path)
        if "duration_seconds" in metadata:
            duration = metadata["duration_seconds"]
            return list(np.arange(0, duration, 5.0))
        else:
            return [0.0]

    # è§£ææ—¶é—´èŒƒå›´
    if "-" in timestamp:
        start_str, end_str = timestamp.split("-")
        start = _time_str_to_seconds(start_str.strip())
        end = _time_str_to_seconds(end_str.strip())
        return list(np.arange(start, end + 1, 1.0))
    else:
        return [_time_str_to_seconds(timestamp)]


def _time_str_to_seconds(time_str: str) -> float:
    """
    å°†æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸ºç§’æ•°

    Examples:
        "4s" -> 4.0
        "1m31s" -> 91.0
        "2m" -> 120.0
    """
    time_str = time_str.lower().strip()

    total_seconds = 0.0

    # è§£æåˆ†é’Ÿ
    if "m" in time_str:
        parts = time_str.split("m")
        minutes = float(parts[0])
        total_seconds += minutes * 60
        time_str = parts[1] if len(parts) > 1 else ""

    # è§£æç§’
    if "s" in time_str:
        time_str = time_str.replace("s", "")

    if time_str:
        total_seconds += float(time_str)

    return total_seconds


def _parse_time_range(timestamp: str, max_duration: float):
    """è§£ææ—¶é—´èŒƒå›´"""
    if "-" in timestamp:
        start_str, end_str = timestamp.split("-")
        start = _time_str_to_seconds(start_str.strip())
        end = _time_str_to_seconds(end_str.strip())
        return start, min(end, max_duration)
    else:
        start = _time_str_to_seconds(timestamp)
        return start, min(start + 5, max_duration)


def _extract_frames(video_path: str, timestamps: List[float]) -> List[dict]:
    """ä»è§†é¢‘ä¸­æå–æŒ‡å®šæ—¶é—´æˆ³çš„å¸§"""
    frames_data = []

    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)

    for timestamp in timestamps:
        frame_idx = int(timestamp * fps)
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()

        if ret:
            frames_data.append({"timestamp_sec": timestamp, "frame": frame})

    cap.release()
    return frames_data


def _draw_boxes(frame, boxes, labels, scores):
    """åœ¨å¸§ä¸Šç»˜åˆ¶è¾¹ç•Œæ¡†ï¼ˆGroundingDINO æ ¼å¼ï¼‰"""
    annotated = frame.copy()
    h, w = frame.shape[:2]

    for box, label, score in zip(boxes, labels, scores):
        # GroundingDINO è¾“å‡ºçš„ box æ˜¯å½’ä¸€åŒ–åæ ‡
        x1, y1, x2, y2 = box
        x1, x2 = int(x1 * w), int(x2 * w)
        y1, y2 = int(y1 * h), int(y2 * h)

        # ç»˜åˆ¶è¾¹ç•Œæ¡†
        cv2.rectangle(annotated, (x1, y1), (x2, y2), (0, 255, 0), 2)

        # ç»˜åˆ¶æ ‡ç­¾
        label_text = f"{label}: {score:.2f}"
        cv2.putText(
            annotated,
            label_text,
            (x1, y1 - 10),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.5,
            (0, 255, 0),
            2,
        )

    return annotated


def _fallback_object_detection(
    frames_data: List[dict], object_prompt: str
) -> List[dict]:
    """
    å½“ GroundingDINO ä¸å¯ç”¨æ—¶çš„ç®€åŒ–æ£€æµ‹æ–¹æ³•
    ï¼ˆåŸºäºæ¨¡æ¿åŒ¹é…æˆ–å…¶ä»–ç®€å• CV æŠ€æœ¯ï¼‰
    """
    # è¿™é‡Œå¯ä»¥å®ç°ç®€å•çš„æ¨¡æ¿åŒ¹é…æˆ–é¢œè‰²æ£€æµ‹
    # ä¸ºäº†ç®€åŒ–ï¼Œè¿™é‡Œåªè¿”å›ä¸€ä¸ªå ä½ç»“æœ
    results = []

    for frame_info in frames_data:
        # ç®€å•ç¤ºä¾‹ï¼šå‡è®¾æˆ‘ä»¬æ€»èƒ½æ‰¾åˆ°ä¸€äº›å¯¹è±¡
        results.append(
            {
                "timestamp_sec": frame_info["timestamp_sec"],
                "detected_object": object_prompt,
                "count": 1,
                "method": "fallback",
                "message": "ä½¿ç”¨ç®€åŒ–æ£€æµ‹æ–¹æ³•ï¼ˆGroundingDINO ä¸å¯ç”¨ï¼‰",
            }
        )

    return results


def _fuzzy_match(search_prompt: str, text: str) -> bool:
    """
    å®½æ¾"æˆ–"é€»è¾‘ - æ£€æŸ¥ search_prompt ä¸­ç”¨ç©ºæ ¼åˆ†å‰²çš„ã€ä»»ä½•ä¸€ä¸ªã€‘å…³é”®è¯æ˜¯å¦å‡ºç°åœ¨ text ä¸­
    
    è¿™ä¸ªå‡½æ•°æ”¯æŒå¤šç§åŒ¹é…ç­–ç•¥ï¼š
    1. ç›´æ¥åŒ…å«ï¼šsearch_prompt å®Œæ•´å‡ºç°åœ¨ text ä¸­
    2. åå‘åŒ…å«ï¼štext å®Œæ•´å‡ºç°åœ¨ search_prompt ä¸­
    3. å…³é”®è¯åŒ¹é…ï¼šsearch_prompt ä¸­çš„ä»»ä½•ä¸€ä¸ªå…³é”®è¯ï¼ˆ>1å­—ç¬¦ï¼‰å‡ºç°åœ¨ text ä¸­
    
    Args:
        search_prompt: æœç´¢æç¤ºè¯ï¼ˆå¯ä»¥æ˜¯å¤šä¸ªå…³é”®è¯ï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼‰
        text: è¦æ£€æŸ¥çš„æ–‡æœ¬
        
    Returns:
        bool: å¦‚æœåŒ¹é…è¿”å› Trueï¼Œå¦åˆ™è¿”å› False
    """
    if not search_prompt:
        return True
    if not text:
        return False

    # ç»Ÿä¸€è½¬ä¸ºå°å†™å¹¶ strip()
    search_prompt_lower = search_prompt.lower().strip()
    text_lower = text.lower().strip()

    # ç­–ç•¥ 1: ç›´æ¥åŒ…å«
    if search_prompt_lower in text_lower:
        return True

    # ç­–ç•¥ 2: åå‘åŒ…å«
    if text_lower in search_prompt_lower:
        return True

    # ç­–ç•¥ 3: å…³é”®è¯åŒ¹é…ï¼ˆ"æˆ–"é€»è¾‘ï¼‰
    keywords = search_prompt_lower.split()
    
    if not keywords:
        return True  # å¦‚æœ prompt strip() åä¸ºç©ºï¼Œç®—åŒ¹é…

    try:
        # æ£€æŸ¥ã€ä»»ä½•ä¸€ä¸ªã€‘å…³é”®è¯ï¼ˆ>1å­—ç¬¦ï¼‰æ˜¯å¦å­˜åœ¨äºæ–‡æœ¬ä¸­
        return any(len(keyword) > 1 and keyword in text_lower for keyword in keywords)
    except Exception:
        return False


def save_video_result(
    task_id: str,
    tool_name: str,
    video_path: str,
    result: str,
):
    """
    ä¿å­˜è§†é¢‘åˆ†æç»“æœåˆ°æ–‡ä»¶

    Args:
        task_id: ä»»åŠ¡ID
        tool_name: å·¥å…·åç§°
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        result: åˆ†æç»“æœ
    """
    output_dir = Path("local_es_data")
    output_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜ä¸»ç»“æœ
    result_file = output_dir / f"{task_id}_video_result.txt"

    try:
        with open(result_file, "w", encoding="utf-8") as f:
            f.write(f"=== Task ID: {task_id} ===\n\n")
            f.write(f"=== å·¥å…· ===\n{tool_name}\n\n")
            f.write(f"=== æ–‡ä»¶ ===\n{video_path}\n\n")
            f.write(f"=== åˆ†æç»“æœ ===\n{result}\n")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜è§†é¢‘ç»“æœå¤±è´¥: {e}")
