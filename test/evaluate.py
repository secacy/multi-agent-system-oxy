#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
äº¬ä¸œå¤šæ™ºèƒ½ä½“æŒ‘æˆ˜èµ› - è¯„ä¼°è„šæœ¬
ç”¨äºå¯¹æ¯”éªŒè¯é›†çš„æ ‡å‡†ç­”æ¡ˆå’Œæ¨¡å‹è¾“å‡ºç»“æœ
"""

import json
import argparse
from pathlib import Path
from typing import Dict, List, Tuple
from collections import defaultdict


def load_jsonl(file_path: str) -> Dict[str, dict]:
    """
    åŠ è½½ JSONL æ–‡ä»¶å¹¶ä»¥ task_id ä¸º key æ„å»ºå­—å…¸
    
    Args:
        file_path: JSONL æ–‡ä»¶è·¯å¾„
        
    Returns:
        {task_id: {å®Œæ•´çš„ä»»åŠ¡æ•°æ®}} å­—å…¸
    """
    data_dict = {}
    with open(file_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                item = json.loads(line)
                task_id = item.get('task_id')
                if not task_id:
                    print(f"âš ï¸  è­¦å‘Š: ç¬¬ {line_num} è¡Œç¼ºå°‘ task_idï¼Œè·³è¿‡")
                    continue
                if task_id in data_dict:
                    print(f"âš ï¸  è­¦å‘Š: task_id '{task_id}' é‡å¤å‡ºç°")
                data_dict[task_id] = item
            except json.JSONDecodeError as e:
                print(f"âŒ é”™è¯¯: ç¬¬ {line_num} è¡Œ JSON è§£æå¤±è´¥: {e}")
                continue
    
    return data_dict


def normalize_answer(answer: str) -> str:
    """
    è§„èŒƒåŒ–ç­”æ¡ˆæ–‡æœ¬ï¼ˆå»é™¤é¦–å°¾ç©ºç™½ï¼‰
    
    Args:
        answer: åŸå§‹ç­”æ¡ˆ
        
    Returns:
        è§„èŒƒåŒ–åçš„ç­”æ¡ˆ
    """
    if answer is None:
        return ""
    return str(answer).strip()


def compare_answers(predicted: str, ground_truth: str) -> bool:
    """
    å¯¹æ¯”é¢„æµ‹ç­”æ¡ˆå’Œæ ‡å‡†ç­”æ¡ˆ
    å½“å‰å®ç°ï¼šä¸¥æ ¼å­—ç¬¦ä¸²åŒ¹é…ï¼ˆå»é™¤é¦–å°¾ç©ºç™½åï¼‰
    
    Args:
        predicted: æ¨¡å‹é¢„æµ‹çš„ç­”æ¡ˆ
        ground_truth: æ ‡å‡†ç­”æ¡ˆ
        
    Returns:
        True è¡¨ç¤ºç­”æ¡ˆæ­£ç¡®ï¼ŒFalse è¡¨ç¤ºé”™è¯¯
    """
    # ä¸¥æ ¼å¯¹æ¯”æ¨¡å¼
    return normalize_answer(predicted) == normalize_answer(ground_truth)


def evaluate(ground_truth_file: str, 
             prediction_file: str, 
             verbose: bool = True,
             show_errors_in_terminal: bool = False,
             only_predicted: bool = False) -> Tuple[float, List[dict], Dict]:
    """
    è¯„ä¼°æ¨¡å‹è¾“å‡ºç»“æœ
    
    Args:
        ground_truth_file: æ ‡å‡†ç­”æ¡ˆæ–‡ä»¶è·¯å¾„
        prediction_file: æ¨¡å‹é¢„æµ‹æ–‡ä»¶è·¯å¾„
        verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†ä¿¡æ¯
        show_errors_in_terminal: æ˜¯å¦åœ¨ç»ˆç«¯æ˜¾ç¤ºé”™è¯¯è¯¦æƒ…
        only_predicted: æ˜¯å¦åªè¯„æµ‹é¢„æµ‹æ–‡ä»¶ä¸­åŒ…å«çš„ task_id
        
    Returns:
        (å‡†ç¡®ç‡, é”™è¯¯åˆ—è¡¨, ç»Ÿè®¡ä¿¡æ¯å­—å…¸)
    """
    print("=" * 80)
    print("ğŸš€ äº¬ä¸œå¤šæ™ºèƒ½ä½“æŒ‘æˆ˜èµ› - è¯„ä¼°ç³»ç»Ÿ")
    print("=" * 80)
    
    # 1. åŠ è½½æ•°æ®
    print(f"\nğŸ“‚ åŠ è½½æ ‡å‡†ç­”æ¡ˆ: {ground_truth_file}")
    ground_truth_dict = load_jsonl(ground_truth_file)
    print(f"   âœ… åŠ è½½äº† {len(ground_truth_dict)} æ¡æ ‡å‡†ç­”æ¡ˆ")
    
    print(f"\nğŸ“‚ åŠ è½½æ¨¡å‹é¢„æµ‹: {prediction_file}")
    prediction_dict = load_jsonl(prediction_file)
    print(f"   âœ… åŠ è½½äº† {len(prediction_dict)} æ¡é¢„æµ‹ç»“æœ")
    
    # 2. æ£€æŸ¥ task_id è¦†ç›–æƒ…å†µ
    ground_truth_ids = set(ground_truth_dict.keys())
    prediction_ids = set(prediction_dict.keys())
    
    missing_ids = ground_truth_ids - prediction_ids
    extra_ids = prediction_ids - ground_truth_ids
    
    # å†³å®šè¯„æµ‹èŒƒå›´
    if only_predicted:
        eval_ids = prediction_ids & ground_truth_ids  # åªè¯„æµ‹ä¸¤è€…äº¤é›†
        print(f"\nğŸ“Œ æ¨¡å¼: åªè¯„æµ‹é¢„æµ‹æ–‡ä»¶ä¸­åŒ…å«çš„ task_id")
        print(f"   è¯„æµ‹èŒƒå›´: {len(eval_ids)} ä¸ªä»»åŠ¡")
        if extra_ids:
            print(f"   âš ï¸  å‘ç° {len(extra_ids)} ä¸ªé¢„æµ‹æ–‡ä»¶ä¸­å¤šä½™çš„ task_idï¼ˆä¸åœ¨æ ‡å‡†ç­”æ¡ˆä¸­ï¼‰")
    else:
        eval_ids = ground_truth_ids  # è¯„æµ‹æ‰€æœ‰æ ‡å‡†ç­”æ¡ˆ
        if missing_ids:
            print(f"\nâš ï¸  è­¦å‘Š: ç¼ºå¤± {len(missing_ids)} ä¸ª task_id çš„é¢„æµ‹ç»“æœ")
            if verbose and len(missing_ids) <= 10:
                print(f"   ç¼ºå¤±çš„ task_id: {list(missing_ids)[:10]}")
        
        if extra_ids:
            print(f"\nâš ï¸  è­¦å‘Š: å‘ç° {len(extra_ids)} ä¸ªå¤šä½™çš„ task_id")
            if verbose and len(extra_ids) <= 10:
                print(f"   å¤šä½™çš„ task_id: {list(extra_ids)[:10]}")
    
    # 3. é€ä¸ªå¯¹æ¯”ç­”æ¡ˆ
    print("\n" + "=" * 80)
    print("ğŸ” å¼€å§‹è¯„ä¼°...")
    print("=" * 80)
    
    correct_count = 0
    total_count = len(eval_ids)
    errors = []
    correct_items = []
    level_stats = defaultdict(lambda: {"correct": 0, "total": 0})
    
    for task_id in eval_ids:
        ground_truth_item = ground_truth_dict[task_id]
        ground_truth_answer = ground_truth_item.get('answer', '')
        level = ground_truth_item.get('level', 'unknown')
        query = ground_truth_item.get('query', '')
        
        # ç»Ÿè®¡å„çº§åˆ«ä»»åŠ¡
        level_stats[level]["total"] += 1
        
        if task_id not in prediction_dict:
            # ç¼ºå¤±é¢„æµ‹ï¼ˆåœ¨ only_predicted æ¨¡å¼ä¸‹ä¸ä¼šå‘ç”Ÿï¼‰
            errors.append({
                'task_id': task_id,
                'level': level,
                'query': query,
                'query_short': query[:100] + '...' if len(query) > 100 else query,
                'ground_truth': ground_truth_answer,
                'predicted': '[ç¼ºå¤±é¢„æµ‹]',
                'status': 'âŒ ç¼ºå¤±'
            })
        else:
            predicted_answer = prediction_dict[task_id].get('answer', '')
            
            if compare_answers(predicted_answer, ground_truth_answer):
                correct_count += 1
                level_stats[level]["correct"] += 1
                correct_items.append({
                    'task_id': task_id,
                    'level': level,
                    'query': query,
                    'answer': ground_truth_answer
                })
            else:
                errors.append({
                    'task_id': task_id,
                    'level': level,
                    'query': query,
                    'query_short': query[:100] + '...' if len(query) > 100 else query,
                    'ground_truth': ground_truth_answer,
                    'predicted': predicted_answer,
                    'status': 'âŒ é”™è¯¯'
                })
    
    # 4. è¾“å‡ºç»“æœ
    accuracy = (correct_count / total_count * 100) if total_count > 0 else 0
    
    print(f"\n{'=' * 80}")
    print(f"ğŸ“Š è¯„ä¼°ç»“æœ")
    print(f"{'=' * 80}")
    print(f"\nâœ… æ€»ä½“å‡†ç¡®ç‡: {correct_count}/{total_count} ({accuracy:.2f}%)")
    
    # æŒ‰çº§åˆ«ç»Ÿè®¡
    print(f"\nğŸ“ˆ åˆ†çº§åˆ«ç»Ÿè®¡:")
    for level in sorted(level_stats.keys()):
        stats = level_stats[level]
        level_acc = (stats["correct"] / stats["total"] * 100) if stats["total"] > 0 else 0
        print(f"   Level {level}: {stats['correct']}/{stats['total']} ({level_acc:.2f}%)")
    
    # é”™è¯¯æ‘˜è¦
    if errors:
        print(f"\nâŒ é”™è¯¯æ•°é‡: {len(errors)} ä¸ª")
        
        # åªåœ¨å¯ç”¨æ—¶æ˜¾ç¤ºç»ˆç«¯è¯¦æƒ…
        if show_errors_in_terminal:
            print(f"\n{'=' * 80}")
            print(f"âŒ é”™è¯¯è¯¦æƒ…")
            print(f"{'=' * 80}\n")
            
            for idx, error in enumerate(errors, 1):
                print(f"[{idx}] Task ID: {error['task_id']}")
                print(f"    çº§åˆ«: Level {error['level']}")
                print(f"    é—®é¢˜: {error['query_short']}")
                print(f"    æ ‡å‡†ç­”æ¡ˆ: {error['ground_truth']}")
                print(f"    æ¨¡å‹ç­”æ¡ˆ: {error['predicted']}")
                print(f"    çŠ¶æ€: {error['status']}")
                print()
        else:
            print(f"   ğŸ’¡ æç¤º: ä½¿ç”¨ --show_errors å‚æ•°å¯åœ¨ç»ˆç«¯æŸ¥çœ‹é”™è¯¯è¯¦æƒ…")
            print(f"   ğŸ’¡ æç¤º: ä½¿ç”¨ --output å‚æ•°å¯å°†å®Œæ•´æŠ¥å‘Šä¿å­˜åˆ°æ–‡ä»¶")
    else:
        print(f"\nğŸ‰ æ­å–œï¼æ‰€æœ‰ç­”æ¡ˆéƒ½æ­£ç¡®ï¼")
    
    print("=" * 80)
    
    # æ„å»ºç»Ÿè®¡ä¿¡æ¯
    stats_info = {
        'total': total_count,
        'correct': correct_count,
        'errors': len(errors),
        'accuracy': accuracy,
        'level_stats': dict(level_stats),
        'missing_count': len(missing_ids) if not only_predicted else 0,
        'extra_count': len(extra_ids),
        'evaluation_mode': 'only_predicted' if only_predicted else 'full'
    }
    
    return accuracy, errors, stats_info


def save_error_report(errors: List[dict], output_file: str):
    """
    ä¿å­˜é”™è¯¯æŠ¥å‘Šåˆ° JSON æ–‡ä»¶
    
    Args:
        errors: é”™è¯¯åˆ—è¡¨
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(errors, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ é”™è¯¯æŠ¥å‘Š (JSON) å·²ä¿å­˜è‡³: {output_file}")


def save_full_report(stats_info: Dict, errors: List[dict], output_file: str):
    """
    ä¿å­˜å®Œæ•´çš„è¯„ä¼°æŠ¥å‘Šåˆ°æ–‡æœ¬æ–‡ä»¶
    
    Args:
        stats_info: ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        errors: é”™è¯¯åˆ—è¡¨
        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("äº¬ä¸œå¤šæ™ºèƒ½ä½“æŒ‘æˆ˜èµ› - å®Œæ•´è¯„ä¼°æŠ¥å‘Š\n")
        f.write("=" * 80 + "\n\n")
        
        # 1. åŸºæœ¬ä¿¡æ¯
        f.write("ğŸ“Š è¯„ä¼°æ‘˜è¦\n")
        f.write("-" * 80 + "\n")
        f.write(f"è¯„ä¼°æ¨¡å¼: {'åªè¯„æµ‹é¢„æµ‹ä»»åŠ¡' if stats_info['evaluation_mode'] == 'only_predicted' else 'å®Œæ•´è¯„æµ‹'}\n")
        f.write(f"æ€»ä»»åŠ¡æ•°: {stats_info['total']}\n")
        f.write(f"æ­£ç¡®æ•°é‡: {stats_info['correct']}\n")
        f.write(f"é”™è¯¯æ•°é‡: {stats_info['errors']}\n")
        f.write(f"æ€»ä½“å‡†ç¡®ç‡: {stats_info['accuracy']:.2f}%\n\n")
        
        # 2. åˆ†çº§åˆ«ç»Ÿè®¡
        f.write("ğŸ“ˆ åˆ†çº§åˆ«ç»Ÿè®¡\n")
        f.write("-" * 80 + "\n")
        for level in sorted(stats_info['level_stats'].keys()):
            level_stat = stats_info['level_stats'][level]
            level_acc = (level_stat['correct'] / level_stat['total'] * 100) if level_stat['total'] > 0 else 0
            f.write(f"Level {level}: {level_stat['correct']}/{level_stat['total']} ({level_acc:.2f}%)\n")
        f.write("\n")
        
        # 3. é”™è¯¯è¯¦æƒ…
        if errors:
            f.write("=" * 80 + "\n")
            f.write(f"âŒ é”™è¯¯è¯¦æƒ… (å…± {len(errors)} ä¸ª)\n")
            f.write("=" * 80 + "\n\n")
            
            for idx, error in enumerate(errors, 1):
                f.write(f"[{idx}] Task ID: {error['task_id']}\n")
                f.write(f"    çº§åˆ«: Level {error['level']}\n")
                f.write(f"    çŠ¶æ€: {error['status']}\n")
                f.write(f"    é—®é¢˜: {error['query']}\n")
                f.write(f"    æ ‡å‡†ç­”æ¡ˆ: {error['ground_truth']}\n")
                f.write(f"    æ¨¡å‹ç­”æ¡ˆ: {error['predicted']}\n")
                f.write("\n" + "-" * 80 + "\n\n")
        else:
            f.write("ğŸ‰ æ­å–œï¼æ‰€æœ‰ç­”æ¡ˆéƒ½æ­£ç¡®ï¼\n")
        
        f.write("=" * 80 + "\n")
        f.write("æŠ¥å‘Šç»“æŸ\n")
        f.write("=" * 80 + "\n")
    
    print(f"ğŸ’¾ å®Œæ•´è¯„ä¼°æŠ¥å‘Š (æ–‡æœ¬) å·²ä¿å­˜è‡³: {output_file}")


def main():
    parser = argparse.ArgumentParser(
        description='äº¬ä¸œå¤šæ™ºèƒ½ä½“æŒ‘æˆ˜èµ›è¯„ä¼°è„šæœ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # åŸºç¡€ç”¨æ³•ï¼ˆé»˜è®¤ï¼šåªè¯„æµ‹é¢„æµ‹ä»»åŠ¡ï¼Œä¿å­˜æŠ¥å‘Šåˆ° outputs/report.txtï¼‰
  python test/evaluate.py
  
  # æŒ‡å®šæ–‡ä»¶è·¯å¾„
  python test/evaluate.py --ground_truth data/validation_set.jsonl --prediction outputs/my_validation_run.jsonl
  
  # è¯„æµ‹æ‰€æœ‰ä»»åŠ¡ï¼ˆåŒ…æ‹¬ç¼ºå¤±çš„é¢„æµ‹ï¼‰
  python test/evaluate.py --no_only_predicted
  
  # åœ¨ç»ˆç«¯æ˜¾ç¤ºé”™è¯¯è¯¦æƒ…
  python test/evaluate.py --show_errors
  
  # è‡ªå®šä¹‰æŠ¥å‘Šè¾“å‡ºè·¯å¾„
  python test/evaluate.py --output outputs/custom_report.txt
  
  # ä¿å­˜é”™è¯¯åˆ—è¡¨ä¸º JSON æ ¼å¼
  python test/evaluate.py --save_errors
        """
    )
    
    parser.add_argument(
        '--ground_truth',
        type=str,
        default='data/validation_set.jsonl',
        help='æ ‡å‡†ç­”æ¡ˆæ–‡ä»¶è·¯å¾„ (é»˜è®¤: data/validation_set.jsonl)'
    )
    
    parser.add_argument(
        '--prediction',
        type=str,
        default='outputs/my_validation_run.jsonl',
        help='æ¨¡å‹é¢„æµ‹æ–‡ä»¶è·¯å¾„ (é»˜è®¤: outputs/my_validation_run.jsonl)'
    )
    
    parser.add_argument(
        '--output',
        type=str,
        default='outputs/report.txt',
        help='å®Œæ•´è¯„ä¼°æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆæ–‡æœ¬æ ¼å¼ï¼ŒåŒ…å«æ‰€æœ‰é”™è¯¯è¯¦æƒ…ï¼‰(é»˜è®¤: outputs/report.txt)'
    )
    
    parser.add_argument(
        '--save_errors',
        action='store_true',
        help='ä¿å­˜é”™è¯¯åˆ—è¡¨åˆ° JSON æ–‡ä»¶'
    )
    
    parser.add_argument(
        '--error_output',
        type=str,
        default='outputs/error_report.json',
        help='é”™è¯¯æŠ¥å‘Š JSON æ–‡ä»¶è¾“å‡ºè·¯å¾„ (é»˜è®¤: outputs/error_report.json)'
    )
    
    parser.add_argument(
        '--show_errors',
        action='store_true',
        help='åœ¨ç»ˆç«¯æ˜¾ç¤ºæ‰€æœ‰é”™è¯¯è¯¦æƒ…'
    )
    
    parser.add_argument(
        '--only_predicted',
        action='store_true',
        default=True,
        help='åªè¯„æµ‹é¢„æµ‹æ–‡ä»¶ä¸­åŒ…å«çš„ task_idï¼ˆé»˜è®¤å¯ç”¨ï¼Œä½¿ç”¨ --no_only_predicted ç¦ç”¨ï¼‰'
    )
    
    parser.add_argument(
        '--no_only_predicted',
        action='store_true',
        help='è¯„æµ‹æ‰€æœ‰æ ‡å‡†ç­”æ¡ˆä¸­çš„ä»»åŠ¡ï¼ˆåŒ…æ‹¬ç¼ºå¤±çš„é¢„æµ‹ï¼‰'
    )
    
    parser.add_argument(
        '--quiet',
        action='store_true',
        help='ç²¾ç®€è¾“å‡ºæ¨¡å¼ï¼ˆä¸æ˜¾ç¤ºè¯¦ç»†è­¦å‘Šï¼‰'
    )
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.ground_truth).exists():
        print(f"âŒ é”™è¯¯: æ ‡å‡†ç­”æ¡ˆæ–‡ä»¶ä¸å­˜åœ¨: {args.ground_truth}")
        return
    
    if not Path(args.prediction).exists():
        print(f"âŒ é”™è¯¯: é¢„æµ‹æ–‡ä»¶ä¸å­˜åœ¨: {args.prediction}")
        return
    
    # å¤„ç† only_predicted å‚æ•°
    # å¦‚æœæŒ‡å®šäº† --no_only_predictedï¼Œåˆ™è¦†ç›–é»˜è®¤å€¼
    only_predicted = args.only_predicted and not args.no_only_predicted
    
    # æ‰§è¡Œè¯„ä¼°
    accuracy, errors, stats_info = evaluate(
        args.ground_truth,
        args.prediction,
        verbose=not args.quiet,
        show_errors_in_terminal=args.show_errors,
        only_predicted=only_predicted
    )
    
    # ä¿å­˜å®Œæ•´æŠ¥å‘Šåˆ°æ–‡æœ¬æ–‡ä»¶
    if args.output:
        Path(args.output).parent.mkdir(parents=True, exist_ok=True)
        save_full_report(stats_info, errors, args.output)
    
    # ä¿å­˜é”™è¯¯åˆ—è¡¨åˆ° JSON æ–‡ä»¶
    if args.save_errors and errors:
        Path(args.error_output).parent.mkdir(parents=True, exist_ok=True)
        save_error_report(errors, args.error_output)


if __name__ == '__main__':
    main()
